{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"development/docker_compose/","title":"Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Docker Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration. These Docker Compose files are available:</p>"},{"location":"development/docker_compose/#development","title":"Development","text":"<ul> <li><code>docker-compose-dev-keycloak.yml</code><ul> <li>Starts the PostgreSQL database, the SecObserve backend, Keycloak and Mailhog</li> <li>The frontend is only started, when the parameter <code>--profile frontend</code> is given</li> <li>A predefined realm calles <code>secobserve</code> is imported on start-up. There is an administrator configured (username: <code>admin</code>, password: <code>admin</code>) and a regular user for Secobserve (username: <code>keycloak_user</code>, password: <code>keycloak</code>).</li> </ul> </li> <li><code>docker-compose-dev-mysql.yml</code><ul> <li>Starts the MySQL database, as well as SecObserve's backend and frontend</li> <li>Backend and frontend are build automatically if necessary and are started in development mode with hot reloading</li> </ul> </li> <li><code>docker-compose-dev.yml</code><ul> <li>Starts the PostgreSQL database, as well as SecObserve's backend and frontend</li> <li>Backend and frontend are build automatically if necessary and are started in development mode with hot reloading</li> </ul> </li> <li><code>docker-compose-playwright.yml</code><ul> <li>Starts the end-to-end tests with Playwright</li> </ul> </li> <li><code>docker-compose-prod-test.yml</code><ul> <li>Starts the PostgreSQL database, as well as SecObserve's backend and frontend</li> <li>Backend and frontend are build automatically if necessary with the production Dockerfiles</li> </ul> </li> <li><code>docker-compose-unittests.yml</code><ul> <li>Starts the unit tests for the backend</li> </ul> </li> <li><code>docker-compose.yml</code><ul> <li>This is a link to <code>docker-compose-dev.yml</code> and is used as a default for the <code>docker compose</code> command</li> </ul> </li> </ul>"},{"location":"development/docker_compose/#production","title":"Production","text":"<p>See the installation guide how to use the productive Docker Compose files.</p> <ul> <li><code>docker-compose-prod-mysql.yml</code></li> <li><code>docker-compose-prod-postgres.yml</code></li> </ul>"},{"location":"getting_started/about/","title":"About SecObserve","text":"<p>SecObserve in a nutshell</p> <p>SecObserve is an open source vulnerability and license management system for software development teams and cloud environments. It supports a variety of open source vulnerability scanners and integrates easily into CI/CD pipelines.</p> <p></p>"},{"location":"getting_started/about/#overview","title":"Overview","text":"<p>The aim of SecObserve is to make vulnerability scanning and vulnerability management as easy as possible for software development projects using open source tools. It consists of 2 major components:</p> <ul> <li> <p>Vulnerability and license management system SecObserve: SecObserve provides the development team with an overview of the results of all vulnerability and license scans for their project, which can be easily filtered and sorted. In the detailed view, the results are displayed uniformly with a wealth of information, regardless of which vulnerability scanner generated them.</p> <p>With the help of automatically executed rules and manual assessments, the vulnerability results can be efficiently evaluated to eliminate irrelevant results and accept risks. This allows the development team to concentrate on fixing the relevant vulnerabilities.</p> </li> <li> <p>GitLab CI templates and GitHub actions: Integrating vulnerability scanners into a CI/CD pipeline can be tedious. Each tool has to be installed differently and is called with different parameters. To avoid having to solve this task all over again, there are repositories with GitLab CI Templates and GitHub Actions. These make the process of integrating vulnerability scanners very simple by providing uniform methods for launching the tools and uniform parameters. The tools are regularly updated in the repositories so that the latest features and bug fixes are always available.</p> <p>All actions and templates run the scanner, upload the results into SecObserve and make the results of the scans available for download as artefacts in JSON format.</p> <p>The sources of the GitHub actions and GitLab CI templates can be found in https://github.com/MaibornWolff/secobserve_actions_templates.</p> </li> </ul> <p></p>"},{"location":"getting_started/about/#workflows","title":"Workflows","text":"<p>There are 2 main workflows when working with SecObserve:</p> <ol> <li> <p>Consuming results of vulnerability scanners: In this workflow, a pipeline runs one or more vulnerability scanners and uploads the results into SecObserve. The developers can then view and assess the observations and take action accordingly.</p> <p>This workflow is typically used in a CI/CD pipeline, but can also be used manually.</p> </li> <li> <p>Consuming SBOMs for license and vulnerability management: Here a Software Bill of Materials (SBOM) is ingested into SecObserve to provide insights into the components and licenses used in the product. It is a precondition for scanning the components for vulnerabilities with the OSV scanner.</p> <p>This workflow can also be used in a CI/CD pipeline or manually.</p> </li> </ol>"},{"location":"getting_started/about/#consuming-results-of-vulnerability-scanners","title":"Consuming results of vulnerability scanners","text":"<pre><code>sequenceDiagram\n    autonumber\n    actor Developer\n    alt Pipeline flow\n    Developer -&gt;&gt; Repository: Check in source\n    activate Repository\n    Repository -&gt;&gt; Pipeline: Start pipeline\n    deactivate Repository\n    activate Pipeline\n    Pipeline -&gt;&gt; Pipeline: Run scanners\n    Pipeline -&gt;&gt; SecObserve: Upload results\n    activate SecObserve\n    SecObserve -&gt;&gt; SecObserve: Apply rules\n    deactivate SecObserve\n    Pipeline -&gt;&gt; SecObserve: Check security gate\n    activate SecObserve\n    SecObserve --&gt;&gt; Pipeline: exit code 0/1\n    deactivate SecObserve\n    Pipeline -&gt;&gt; Pipeline: Stop or continue\n    Pipeline --&gt;&gt; Developer: Feedback\n    deactivate Pipeline\n    else Manual flow\n    Developer -&gt;&gt; SecObserve: Upload scan results\n    activate SecObserve\n    SecObserve -&gt;&gt; SecObserve: Apply rules\n    deactivate SecObserve\n    end\n    Developer -&gt;&gt; SecObserve: View observations\n    Developer -&gt;&gt; SecObserve: Assess observations\n    Developer -&gt;&gt; Developer: Implement fixes\n    Developer -&gt;&gt; Repository: Check in source ...</code></pre> <ol> <li>A developer implements a feature and checks in his code to the repository</li> <li>The repository starts a pipeline for the change</li> <li>The pipeline runs one or more of the supported vulnerability scanners. To make integration easy, SecObserve provides predefined GitHub actions and GitLab templates for the most relevant scanners, see GitHub actions and GitLab CI templates.</li> <li>The scanners store their results in files, which are uploaded into SecObserve.</li> <li>SecObserve applies rules to adjust severity and status of observations during the upload process.</li> <li>The pipeline can call SecObserve to check the status of the security gate.</li> <li>SecObserve returns an exit code to the pipeline: <ul> <li><code>1</code> if the security gate has failed</li> <li><code>0</code> if the security gate has passed or is disabled</li> </ul> </li> <li>The pipeline can stop or continue based on the exit code, depending on the configuration of the check step. Default is to stop the pipeline if the security gate has failed.</li> <li>The developer can see the result of the pipeline.</li> <li>Alternatively, the developer can upload the results manually into SecObserve ...</li> <li>... and SecObserve applies the rules to adjust severity and status of observations.</li> <li>The developer can now look at the observations in SecObserve, to see what has changed ...</li> <li>... and if necessary assess observations to change their status (eg. false positive or risk accepted) or severity.</li> <li>If fixes are needed to close vulnerabilities, the developer will implement the fixes ...</li> <li>... and check them in to the repository. Now the cycle starts again.</li> </ol>"},{"location":"getting_started/about/#consuming-sboms-for-license-and-vulnerability-management","title":"Consuming SBOMs for license and vulnerability management","text":"<pre><code>sequenceDiagram\n    autonumber\n    actor Developer\n    alt Pipeline flow\n    Developer -&gt;&gt; Pipeline: Start pipeline\n    activate Pipeline\n    Pipeline -&gt;&gt; Pipeline: Generate SBOM\n    Pipeline -&gt;&gt; SecObserve: Upload SBOM\n    deactivate Pipeline\n    else Manual flow\n    Developer -&gt;&gt; SecObserve: Upload SBOM\n    end\n    Note right of Developer: License management\n    SecObserve -&gt;&gt; SecObserve: Apply license policy\n    Developer -&gt;&gt; SecObserve: View licenses\n    Developer -&gt;&gt; SecObserve: Adjust license policy\n    Note right of Developer: Vulnerability management\n    Developer -&gt;&gt; SecObserve: Run manual OSV scan\n    SecObserve -&gt;&gt; SecObserve: Run nightly OSV scan</code></pre> <ol> <li>A developer starts a pipeline ...</li> <li>... to generate a Software Bill of Materials (SBOM) for the project.</li> <li>The pipeline uploads the SBOM into SecObserve.</li> <li>Alternatively, the developer can upload the SBOM manually into SecObserve.</li> <li>SecObserve applies the license policy to the SBOM to check for license compliance.</li> <li>The developer can view the licenses and their evaluation results in SecObserve ...</li> <li>... and adjust the license policy if necessary.</li> <li>The developer can run a manual OSV scan to check for vulnerabilities in the components of the SBOM.</li> <li>Alternatively, SecObserve can run a nightly OSV scan to check for vulnerabilities in the components of the SBOM.</li> </ol>"},{"location":"getting_started/acknowledgements/","title":"Acknowledgements","text":"<ul> <li>A huge shoutout to all the fabulous Open Source libraries that made this project possible. Too many to mention them all here, but they are listed in the current SBOM.</li> <li>The binoculars logo has been derived from the National Park Service's Birding/Wildlife Viewing icon, published under the BSD-3-Clause license.</li> </ul>"},{"location":"getting_started/anatomy_of_an_observation/","title":"Anatomy of an observation","text":""},{"location":"getting_started/anatomy_of_an_observation/#observation","title":"Observation","text":"<ul> <li>The Severity can have 3 sources: <ul> <li>Initially the parser sets a severity, based on the incoming data. </li> <li>If there is a rule configured that matches the observation, it overrides the severity set by the parser.</li> <li>When a user assesses the observation and sets a different severity, this severity from the assessment overrides the severity set by a rule and the severity set by the parser.</li> </ul> </li> <li>The initial Status set by an import is <code>Open</code>. It will be set to <code>Resolved</code> if the same observation is not found in a subsequent import. On the other hand, resolved observations are set back to the status <code>Open</code> if they reappear in a later import. As for the severity, if a rule matches the observation or a user sets a different status in an assessment, these changes will override the status set by the import.</li> <li>Title and Description are short and long explanations what the observation is about.</li> <li>Scanners might suggest a Recommendation (not shown in the screenshot).</li> </ul>"},{"location":"getting_started/anatomy_of_an_observation/#vulnerability","title":"Vulnerability","text":"<p>(not shown in the screenshot)</p> <p>Vulnerability data can comprise a Vulnerability Id like a CVE or GHSA, a CVSSv3 score / CVSSv3 vector and/or CVSSv4 score / CVSSv4 vector as well as a CWE number.</p>"},{"location":"getting_started/anatomy_of_an_observation/#origins","title":"Origins","text":"<p>An observation can be found at different origins:</p> <ul> <li>Service: A service is a self-contained piece of functionality within a product. This can be something like frontend or backend or the name of a microservice.</li> <li>Component: Typically a library (Maven, NPM, PyPI, ...) or a program installed in a docker image, identified by name and version.</li> <li>Docker image: Name and tag of a Docker image, where the observation was found.</li> <li>Endpoint: The URL of a web address.</li> <li>Source file: Path and name of a source file, start and end lines are optional. The source file will be shown as a link to the source in the repository, if a Repository prefix has been configured in the product.</li> <li>Cloud: The name of the cloud provider, account (AWS) or subscription (Azure) or project (GCP), resource and resource type, where the observation was found.</li> </ul>"},{"location":"getting_started/anatomy_of_an_observation/#log","title":"Log","text":"<p>Every time either the severity or the status get changed by an import or an assessment, this event is recorded in the Observation Log together with a comment.</p>"},{"location":"getting_started/anatomy_of_an_observation/#potential-duplicates","title":"Potential duplicates","text":"<p>(not shown in the screenshot)</p> <p>If an observation has potential duplicates, they are listed here and can be marked as duplicates.</p>"},{"location":"getting_started/anatomy_of_an_observation/#metadata","title":"Metadata","text":"<p>Some information about how the observation was created or updated.</p>"},{"location":"getting_started/anatomy_of_an_observation/#references","title":"References","text":"<p><code>References</code> are links to further information about the observation. They are imported with the observation.</p>"},{"location":"getting_started/anatomy_of_an_observation/#evidences","title":"Evidences","text":"<p><code>Evidences</code> are extracts from the scan reports showing the basis on which the observation was created.</p>"},{"location":"getting_started/architecture/","title":"Architecture","text":""},{"location":"getting_started/architecture/#frontend","title":"Frontend","text":"<p>The frontend is a single page application (SPA), implemented with TypeScript, React and the React-Admin framework. The page is delivered by a nginx server.</p>"},{"location":"getting_started/architecture/#backend","title":"Backend","text":"<p>The backend is implemented with Python and Django / Django Rest Framework. A Gunicorn server delivers the REST API that is used by the frontend to show and manipulate data and by CI/CD pipelines to upload scan results.</p>"},{"location":"getting_started/architecture/#database","title":"Database","text":"<p>Currently MySQL and PostgreSQL are supported as databases.</p>"},{"location":"getting_started/configuration/","title":"Configuration","text":""},{"location":"getting_started/configuration/#deployment","title":"Deployment","text":"<p>A part of the configuration is done with environment variables, which need to be set when deploying SecObserve. How this is done depends on the deployment method, see Installation.</p>"},{"location":"getting_started/configuration/#backend","title":"Backend","text":"Environment variable Optionality Description <code>ADMIN_USER</code> mandatory Username of the administration user. The user will be created at the fist start of the backend. <code>ADMIN_EMAIL</code> optional E-Mail of the administration user. <code>ADMIN_PASSWORD</code> optional Initial password of the admin user. If it is not set, a random password will be created during startup and shown in the log. <code>ALLOWED_HOSTS</code> mandatory Hostnames of the backend, see Django settings ALLOWED_HOSTS. This can be a comma-separated list of hostnames. <code>CORS_ALLOWED_ORIGINS</code> mandatory URL of the frontend that is authorized to make cross-site HTTP requests. This can be a comma-separated list of URLs. <code>DATABASE_HOST</code> mandatory Which host to use when connecting to the database. <code>DATABASE_DB</code> mandatory The name of the database to use. <code>DATABASE_PORT</code> mandatory The port to use when connecting to the database. <code>DATABASE_USER</code> mandatory The username to use when connecting to the database. <code>DATABASE_PASSWORD</code> mandatory The password to use when connecting to the database. <code>DATABASE_ENGINE</code> mandatory The database backend to use. Supported database backends are <code>django.db.backends.mysql</code> and <code>django.db.backends.postgresql</code> <code>MYSQL_AZURE</code> optional Must be set if Azure Database for MySQL is used, to use the necessary SSL certificate. For MySQL Flexible Server it needs to have the value <code>flexible</code>, for MySQL Single Server the the value needs to be <code>single</code>. See Connect using mysql command-line client with TLS/SSL and Configure SSL connectivity in your application to securely connect to Azure Database for MySQL. <code>DJANGO_SECRET_KEY</code> mandatory A secret key for a particular Django installation. This is used to provide cryptographic signing, and should be set to a unique, unpredictable value with at least 50 characters, see Django settings SECRET_KEY. <code>FIELD_ENCRYPTION_KEY</code> mandatory Key to encrypt fields like the JWT secret. See Generating an Encryption Key how to generate the key. <code>GUNICORN_WORKERS</code> optional Number of worker processes for the Gunicorn web server, see Gunicorn documentation. Default is 3. <code>GUNICORN_THREADS</code> optional Number of worker threads for the Gunicorn web server, default is 10. <code>GUNICORN_LIMIT_REQUEST_FIELD_SIZE</code> optional Limits the allowed size of an HTTP request header field, default is 16380. <code>OIDC_AUTHORITY</code> mandatory The authority is a URL that hosts the OpenID configuration well-known endpoint. <code>OIDC_CLIENT_ID</code> mandatory The client ID is the unique Application (client) ID assigned to your app by the OpenID Connect provider when the app was registered. <code>OIDC_USERNAME</code> mandatory The claim that contains the username to find or create the user. <code>OIDC_FIRST_NAME</code> mandatory The claim that contains the first name of the user. <code>OIDC_LAST_NAME</code> mandatory The claim that contains the last name of the user. <code>OIDC_FULL_NAME</code> mandatory The claim that contains the full name of the user. <code>OIDC_EMAIL</code> mandatory The claim that contains the email address of the user. <code>OIDC_GROUPS</code> optional The claim that contains the groups of the user."},{"location":"getting_started/configuration/#frontend","title":"Frontend","text":"Environment variable Optionality Description <code>API_BASE_URL</code> mandatory URL where to find the backend API, e.g. <code>https:\\\\secobserve-backend.example.com/api</code>. <code>OIDC_ENABLE</code> mandatory <code>true</code>: OpenID Connect authentication is active, <code>false</code>: otherwise. <code>OIDC_AUTHORITY</code> mandatory The authority is a URL that hosts the OpenID Connect configuration well-known endpoint. <code>OIDC_CLIENT_ID</code> mandatory The client ID is the unique Application (client) ID assigned to your app by the OpenID Connect provider when the app was registered. <code>OIDC_REDIRECT_URI</code> mandatory The redirect URI is the URI the identity provider will send the security tokens back to. To be set with the URL of the frontend. <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> mandatory The post logout redirect URI is the URI that will be called after logout. To be set with the URL of the frontend. <code>OIDC_SCOPE</code> optional OpenID Connect (OIDC) scopes are used by an application during authentication to authorize access to a user's details, like name or email. If the variable is not set, the standard scopes <code>openid profile email</code> will be used. <p>All the <code>OIDC_*</code> environment variables are needed for technical reasons. If <code>OIDC_ENABLE</code> is set to <code>false</code>, the other <code>OIDC_*</code> environment variables can be set to <code>dummy</code> or something similar.</p> <p>More about the configuration for different OpenID Connect providers can be found in OpenID Connect authentication.</p>"},{"location":"getting_started/configuration/#admininistration-in-secobserve","title":"Admininistration in SecObserve","text":"<p>Other parts of the configuration are done in the administration interface of SecObserve under <code>Settings</code>, which can only be accessed by users with the role <code>Superuser</code>.</p> <p></p> <p>The entries shall be checked and adjusted if necessary after installing SecObserve.</p>"},{"location":"getting_started/data_model/","title":"Data model","text":""},{"location":"getting_started/data_model/#vulnerability-management","title":"Vulnerability Management","text":"<pre><code>erDiagram\n    Product_Group |o--o{ Product : has\n    Product ||--o{ Observation : has\n    Product ||--o{ Branch_Version : has\n    Product ||--o{ Service : has\n    Product ||--o{ Vulnerability_Check : has\n    Product ||--o{ Product_Rule : has\n    Product ||--o{ API_Configuration : has\n    Product ||--o{ Product_Member : has\n    Product ||--o{ Product_Authorization_Group_Member : has\n    Parser ||--o{ Observation: discovered_by\n    Observation }o--o| Branch_Version: found_in\n    Observation ||--|{ Observation_Log : has\n    Observation ||--|{ Reference : has\n    Observation ||--|{ Evidence : has\n    General_Rule</code></pre>"},{"location":"getting_started/data_model/#product-group","title":"Product Group","text":"<p>A <code>Product Group</code> is a collection of products. It is used to group products that belong together, e.g. because they are part of the same project. In the database, the product groups are stored in the table <code>Product</code> with the flag <code>is_product_group</code> set to <code>true</code>.</p>"},{"location":"getting_started/data_model/#product","title":"Product","text":"<p>A <code>Product</code> is the representation of the system that is checked for vulnerabilities.</p>"},{"location":"getting_started/data_model/#observation","title":"Observation","text":"<p>An <code>Observation</code> is something that has been discovered by a vulnerability scanner. Not every observation is actually a vulnerability. An assessment can show it is e.g. a false positive or not applicable in the current context.</p> <p>Every <code>Observation</code> belongs to exactly one product.</p>"},{"location":"getting_started/data_model/#branch-version","title":"Branch / Version","text":"<p>Software development often uses branches in the source code repository and software is often available in multiple versions. Vulnerability scanners can run for different branches / versions of a product and observations can be viewed and managed by branch / version. See more in branches and Versions.</p>"},{"location":"getting_started/data_model/#service","title":"Service","text":"<p>A <code>Service</code> is a self-contained piece of functionality of a product. Can be something like a microservice or <code>backend</code> or <code>frontend</code>.   </p>"},{"location":"getting_started/data_model/#vulnerability-check","title":"Vulnerability Check","text":"<p>An import for one product, one branch / version and one file name resp. one API configuration is a so-called vulnerability check. See more in Import algorithm.</p>"},{"location":"getting_started/data_model/#parser","title":"Parser","text":"<p>SecObserve can parse a variety of data formats, written by vulnerability scanners. Besides file-based parsers, SecObserve implements API-based parsers as well, which get data directly from a system via REST calls. See more about vulnerability scanners and parsers on Supported scanners.</p>"},{"location":"getting_started/data_model/#observation-log","title":"Observation Log","text":"<p>Every change of the severity or the status of an observation is recorded in the <code>Observation Log</code>.</p>"},{"location":"getting_started/data_model/#reference","title":"Reference","text":"<p><code>References</code> are links to further information about the observation. They are imported with the observation.</p>"},{"location":"getting_started/data_model/#evidence","title":"Evidence","text":"<p><code>Evidences</code> are extracts from the scan reports showing the basis on which the observation was created.</p>"},{"location":"getting_started/data_model/#product-rule-general-rule","title":"Product Rule / General Rule","text":"<p>Rules can change the severity or the status of an observation can be changed during the import. An example would be to set a risk acceptance automatically for observations that shall not be fixed. <code>General Rules</code> will be applied for all products, while <code>Product Rules</code> are only valid for one product. See more about rules on Rule engine.</p>"},{"location":"getting_started/data_model/#api-configuration","title":"API Configuration","text":"<p>Parsers who get data from vulnerability scanners via a REST API need a configuration how to access the API (URL, API key, ...). The <code>API Configuration</code> is set per product.</p>"},{"location":"getting_started/data_model/#product-member","title":"Product Member","text":"<p><code>Product Members</code> define which users have access to a product. Depending on the role of a user for a product, they have more or less functionality available, see more on Users and permissions.</p>"},{"location":"getting_started/data_model/#product-authorization-group-member","title":"Product Authorization Group Member","text":"<p><code>Product Authorization Group Members</code> define which authorization groups have access to a product. Depending on the role, the users of the authorization group have more or less functionality available, see more on Users and permissions.</p>"},{"location":"getting_started/data_model/#license-management","title":"License Management","text":"<pre><code>erDiagram\n    Product ||--o{ Branch_Version : has\n    Product ||--o{ License_Component : has\n    Branch_Version }o--o| License_Policy : references\n    Product }o--o| License_Policy : references\n    Product_Group }o--o| License_Policy : references\n    License_Component }o--o| License : references\n    License_Component ||--|{ License_Component_Evidence : has\n    License_Policy }o--o| License_Policy : parent\n    License_Policy ||--o{ License_Policy_Item : has\n    License_Policy ||--o{ License_Policy_Member : has\n    License_Policy ||--o{ License_Policy_Authorization_Group_Member : has\n    License_Group }o--o{ License : references\n    License_Group ||--o{ License_Group_Member : has\n    License_Group ||--o{ License_Group_Authorization_Group_Member : has\n    License_Policy_Item }o--o| License : references\n    License_Policy_Item }o--o| License_Group : references</code></pre>"},{"location":"getting_started/data_model/#license","title":"License","text":"<p>The Linux Foundation gathers a list of commonly found licenses and exceptions used for open source and other collaborative software. The list is called SPDX License List and is imported daily into SecObserve.</p>"},{"location":"getting_started/data_model/#license-component","title":"License Component","text":"<p>A <code>License Component</code> is a library or package used in a product that is licensed under a specific license and has an evaluation of the license according to a license policy. Depending on the license information in the scan report, there are 3 different types of licenses:</p> <ul> <li>a license with a known SPDX identifier</li> <li>a license expression, if the license expression in the scan report is valid according to the SPDX specification and consists only of known SPDX identifiers</li> <li>a non-spdx license string in all other cases</li> </ul>"},{"location":"getting_started/data_model/#license-component-evidence","title":"License Component Evidence","text":"<p><code>License Component Evidences</code> are extracts from the scan reports showing the basis on which the license component was created.</p>"},{"location":"getting_started/data_model/#license-policy","title":"License Policy","text":"<p>A <code>License Policy</code> defines the rules for the usage of licenses in a product. It can define which licenses are allowed, which are forbidden, and which need a review.</p> <p>A <code>License Policy</code> can have another license policy as a <code>Parent</code>. If a license policy has a parent, the rules of the parent are also valid for the child policy, but existing rules of the parent can be overriden and new rules can be added. </p>"},{"location":"getting_started/data_model/#license-policy-item","title":"License Policy Item","text":"<p>A <code>License Policy Item</code> is a single rule in a license policy. It can be a rule for a specific license, a rule for a license group or a rule for a non-spdx license string, e.g. a license that is not in the SPDX list or a license expression.</p>"},{"location":"getting_started/data_model/#license-policy-member","title":"License Policy Member","text":"<p><code>License Policy Members</code> define which users have access to a license policy, either read-only or as a manager.</p>"},{"location":"getting_started/data_model/#license-policy-authorization-group-member","title":"License Policy Authorization Group Member","text":"<p><code>License Policy Authorization Group Members</code> define which authorization groups have access to a license policy, either read-only or as a manager.</p>"},{"location":"getting_started/data_model/#license-group","title":"License Group","text":"<p>A <code>License Group</code> is a collection of licenses with similar license conditions. There is a predefined list of license groups, taken from the classification of the Blue Oak Council.</p>"},{"location":"getting_started/data_model/#license-group-member","title":"License Group Member","text":"<p><code>License Group Members</code> define which users have access to a license group, either read-only or as a manager.</p>"},{"location":"getting_started/data_model/#license-group-authorization-group-member","title":"License Group Authorization Group Member","text":"<p><code>License Group Authorization Group Members</code> define which authorization groups have access to a license group, either read-only or as a manager.</p>"},{"location":"getting_started/features/","title":"Features","text":""},{"location":"getting_started/features/#vulnerability-management","title":"Vulnerability Management","text":"Feature Supported Flexible data model with products, product groups and services Observations with a wide range of information Multiple branches and versions per product Automatic resolution of fixed vulnerabilities Identification and management of duplicates Manual assessment of severity and status Rule based assessment of severity and status Security gates Actual and weekly metrics Configurable expiry for accepted risks"},{"location":"getting_started/features/#license-management","title":"License Management","text":"Feature Supported Import of components with license information from CycloneDX and SPDX SBOMs Flexible license policies to evaluate the impact of different license conditions Organize licenses with similar conditions in license groups"},{"location":"getting_started/features/#integrations","title":"Integrations","text":"Feature Supported Import from many SAST, SCA, DAST, infrastructure and secrets scanners GitLab CI integration of scanners with predefined templatesGitHub integration of scanners with predefined actions Data enrichment from Exploit Prediction Scoring System (EPSS) Data enrichment with exploit information Always up-to-date SPDX licenses License groups generated from ScanCode LicenseDB data Direct link to source code Export vulnerabilities to issue trackers (Jira, GitLab, GitHub) Import/export vulnerabilities from/to VEX documents (CSAF, OpenVEX) Vulnerability scanning from OSV database Export of data to Microsoft Excel and CSV Export metrics to CodeCharta Notifications to Microsoft Teams, Slack and email Links to additional information about vulnerabilities and components REST API"},{"location":"getting_started/features/#access-control","title":"Access Control","text":"Feature Supported Built-in user management OpenID Connect integration Internal, external and admin users Authorization groups Role-based access control"},{"location":"getting_started/features/#installation-and-upgrading","title":"Installation and Upgrading","text":"Feature Supported Installation with Docker Compose Supported databases: PostgreSQL and MySQL Flexible configuration Automatic database migration during upgrades"},{"location":"getting_started/installation/","title":"Installation","text":"<p>Warning</p> <p>Both provided installation options serve as templates for productive use only. Even though they can run out of the box, they will need proper configuration for the requirements of the environment they will be installed in. This includes additional hardening and security measures.</p>"},{"location":"getting_started/installation/#docker-compose","title":"Docker Compose","text":"<p>SecObserve provides 2 Docker Compose files as templates for productive use: <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code>. Both start Traefik as an edge router as well as the SecObserve frontend and backend plus a database (either MySQL or PostgreSQL).</p> <p>Without any changes to the Docker Compose file, 3 URL's are available:</p> <ul> <li>Frontend: http://secobserve.localhost</li> <li>Backend: http://secobserve-backend.localhost (base URL)</li> <li>Traefik: http://traefik.localhost (dashboard)</li> </ul> docker-compose-prod-postgres.yml<pre><code>name: \"secobserve_prod\"\n\nvolumes:\n  prod_postgres_data:\n\nnetworks:\n  traefik:\n  database:\n\nservices:\n\n  traefik:\n    image: \"traefik:v3.5.4\"\n    container_name: \"prod_traefik\"\n    command:\n      - \"--log.level=INFO\"\n      - \"--api.dashboard=true\"\n      - \"--providers.docker=true\"\n      - \"--providers.docker.exposedbydefault=false\"\n      - \"--providers.docker.network=secobserve_prod_traefik\"\n      - \"--entrypoints.web.address=:80\"\n    labels:\n      - \"traefik.enable=true\"\n      # - \"traefik.http.middlewares.traefik-ipallowlist.ipallowlist.sourcerange=172.18.0.1/24\"\n      # - \"traefik.http.routers.api.middlewares=traefik-ipallowlist@docker\"\n      - \"traefik.http.routers.api.entrypoints=web\"\n      - \"traefik.http.routers.api.rule=Host(`traefik.localhost`)\"\n      - \"traefik.http.routers.api.service=api@internal\"\n    ports:\n      - \"80:80\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n    networks:\n      - default\n      - traefik\n\n  frontend:\n    image: maibornwolff/secobserve-frontend:1.41.0\n    container_name: \"prod_secobserve_frontend\"\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.frontend.rule=Host(`secobserve.localhost`)\"\n      - \"traefik.http.routers.frontend.entrypoints=web\"\n      - \"traefik.http.services.frontend.loadbalancer.server.port=3000\"\n    environment:\n      API_BASE_URL: ${SO_API_BASE_URL:-http://secobserve-backend.localhost/api}\n      OIDC_ENABLE: ${SO_OIDC_ENABLE:-false}\n      OIDC_AUTHORITY: ${SO_OIDC_AUTHORITY:-dummy}\n      OIDC_CLIENT_ID: ${SO_OIDC_CLIENT_ID:-dummy}\n      OIDC_REDIRECT_URI: ${SO_OIDC_REDIRECT_URI:-http://secobserve.localhost}\n      OIDC_POST_LOGOUT_REDIRECT_URI: ${SO_OIDC_POST_LOGOUT_REDIRECT_URI:-http://secobserve.localhost}\n      OIDC_SCOPE: ${SO_OIDC_SCOPE:-openid profile email}\n    networks:\n      - traefik\n\n  backend:\n    image: maibornwolff/secobserve-backend:1.41.0\n    container_name: \"prod_secobserve_backend\"\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.backend.rule=Host(`secobserve-backend.localhost`)\"\n      - \"traefik.http.routers.backend.entrypoints=web\"\n    depends_on:\n      - postgres\n    environment:\n      # --- Admin user ---\n      ADMIN_USER: ${SO_ADMIN_USER:-admin}\n      ADMIN_PASSWORD: ${SO_ADMIN_PASSWORD:-admin}\n      ADMIN_EMAIL: ${SO_ADMIN_EMAIL:-admin@example.com}\n      # --- Gunicorn ---\n      GUNICORN_WORKERS: ${SO_GUNICORN_WORKERS:-3}\n      GUNICORN_THREADS: ${SO_GUNICORN_THREADS:-10}\n      GUNICORN_LIMIT_REQUEST_FIELD_SIZE: ${SO_GUNICORN_LIMIT_REQUEST_FIELD_SIZE:-16380}\n      # --- Database ---\n      DATABASE_ENGINE: ${SO_DATABASE_ENGINE:-django.db.backends.postgresql}\n      DATABASE_HOST: ${SO_DATABASE_HOST:-postgres}\n      DATABASE_PORT: ${SO_DATABASE_PORT:-5432}\n      DATABASE_DB: ${SO_DATABASE_DB:-secobserve}\n      DATABASE_USER: ${SO_DATABASE_USER:-secobserve}\n      DATABASE_PASSWORD: ${SO_DATABASE_PASSWORD:-secobserve}\n      # --- Security ---\n      ALLOWED_HOSTS: ${SO_ALLOWED_HOSTS:-secobserve-backend.localhost}\n      CORS_ALLOWED_ORIGINS: ${SO_CORS_ALLOWED_ORIGINS:-http://secobserve.localhost}\n      DJANGO_SECRET_KEY: ${SO_DJANGO_SECRET_KEY:-NxYPEF5lNGgk3yonndjSbwP77uNJxOvfKTjF5aVBqsHktNlf1wfJHHvJ8iifk32r}\n      FIELD_ENCRYPTION_KEY: ${SO_FIELD_ENCRYPTION_KEY:-DtlkqVb3wlaVdJK_BU-3mB4wwuuf8xx8YNInajiJ7GU=}\n      # --- OpenID Connect ---\n      OIDC_AUTHORITY: ${SO_OIDC_AUTHORITY:-}\n      OIDC_CLIENT_ID: ${SO_OIDC_CLIENT_ID:-}\n      OIDC_USERNAME: ${SO_OIDC_USERNAME:-}\n      OIDC_FIRST_NAME: ${SO_OIDC_FIRST_NAME:-}\n      OIDC_LAST_NAME: ${SO_OIDC_LAST_NAME:-}\n      OIDC_FULL_NAME: ${SO_OIDC_FULL_NAME:-}\n      OIDC_EMAIL: ${SO_OIDC_EMAIL:-}\n      OIDC_GROUPS: ${SO_OIDC_GROUPS:-}\n    networks:\n      - traefik\n      - database\n\n  postgres:\n    image: postgres:15.14-alpine\n    container_name: \"prod_postgres\"\n    environment:\n      POSTGRES_DB: ${SO_POSTGRES_DB:-secobserve}\n      POSTGRES_USER: ${SO_POSTGRES_USER:-secobserve}\n      POSTGRES_PASSWORD: ${SO_POSTGRES_PASSWORD:-secobserve}\n    volumes:\n      - prod_postgres_data:/var/lib/postgresql/data\n    networks:\n      - database\n</code></pre>"},{"location":"getting_started/installation/#configuration-for-traefik","title":"Configuration for Traefik","text":"<ul> <li>The Traefik dashboard should either be configured with authentication or disabled, see The Dashboard.</li> <li>Encrypted communiction should be configured for frontend and backend. Traefik supports given certificates and automatic configuration with Let's Encrypt, see HTTPS &amp; TLS.</li> </ul>"},{"location":"getting_started/installation/#configuration-for-secobserve","title":"Configuration for SecObserve","text":"<p>The Docker Compose file sets default values for the SecObserve configuration, so that the containers can run out of the box. All default values can be overriden, by setting respective environment variables in the shell before starting Docker Compose. To avoid name collisions, the environment variables in the shell need to have a <code>SO_</code> prefix in front of the name as it is stated in Configuration.</p> <p>Some values should be changed for productive use, to avoid using the default values for secrets:</p> <ul> <li><code>SO_ADMIN_PASSWORD</code></li> <li><code>SO_DATABASE_PASSWORD</code></li> <li><code>SO_DJANGO_SECRET_KEY</code></li> <li><code>SO_FIELD_ENCRYPTION_KEY</code></li> </ul>"},{"location":"getting_started/installation/#startup","title":"Startup","text":"<ul> <li>The database structure is initialized with the first start of the backend container.</li> <li>The URLs for frontend and backend are available after approximately 30 seconds, after the healthcheck of the containers has been running for the first time.</li> </ul>"},{"location":"getting_started/installation/#kubernetes","title":"Kubernetes","text":"<p>SecObserve provides a Helm chart as a template for productive use. The default values will work if the release name is <code>secobserve</code> and the frontend will be accessible with https://secobserve.dev/.</p>"},{"location":"getting_started/installation/#database","title":"Database","text":"<p>The PostgreSQL database is provided by Bitnami's Helm chart. Bitnami doesn't provide updates for their free tier anymore, see Upcoming changes to the Bitnami Catalog and the Docker image is pulled from the <code>bitnamilegacy</code> repository. </p> <p>This is ok to test the Kubernetes installation, but not suitable for production use. A productive environment has to use an update-to-date database, e.g. installed as an operator like CloudNativePG or a managed service of a cloud provider.</p> <p>If the provided database is used and the chart is installed with a release name different from <code>secobserve</code>, all occurrences of <code>secobserve-postgresql</code> in the chart have to be changed to <code>&lt;release_name&gt;-postgresql</code>.</p>"},{"location":"getting_started/installation/#secrets","title":"Secrets","text":"<p>Three values are read from a secret, which has to be set up manually before installing the chart:</p> <ul> <li><code>ADMIN_PASSWORD</code></li> <li><code>DJANGO_SECRET_KEY</code></li> <li><code>FIELD_ENCRYPTION_KEY</code></li> </ul> <p>The command to setup the secret can look like this:</p> <pre><code>kubectl create secret generic secobserve-secrets \\\n    --namespace ... \\\n    --from-literal=password='...' \\\n    --from-literal=django_secret_key='...' \\\n    --from-literal=field_encryption_key='...'\n</code></pre> <p>See Configuration for more information how to set these values.</p>"},{"location":"getting_started/upgrading/","title":"Upgrading","text":""},{"location":"getting_started/upgrading/#generic-upgrade-procedure","title":"Generic upgrade procedure","text":"<ul> <li> <p>Frontend and backend shall always be started with the same version number. </p> </li> <li> <p>The Docker Compose <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code> in the GitHub repository always use the most recent released version of SecObserve.</p> </li> <li> <p>The database structure will automatically be updated to the reflect the latest changes, when the backend container gets started. Always make a backup of your database before upgrading, in case something should go wrong.</p> </li> <li> <p>There will be specific upgrade instructions if necessary, e.g. when there are new configuration parameters.</p> </li> </ul>"},{"location":"getting_started/upgrading/#release-1410","title":"Release 1.41.0","text":"<p>Breaking changes</p> <ul> <li>The field <code>[origin_]component_purl_namespace</code> has been removed from the APIs for <code>observations</code>, <code>license_components</code> and <code>components</code>. Users of the API shall parse the <code>[origin_]component_purl</code> if they need any of its attributes.</li> </ul>"},{"location":"getting_started/upgrading/#release-1380","title":"Release 1.38.0","text":"<p>Noteable change</p> <ul> <li>Microsoft is rotating the root certificate for the flexible Azure Database for MySQL see https://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-root-certificate-rotation. This release contains the new certificates.</li> </ul>"},{"location":"getting_started/upgrading/#release-1370","title":"Release 1.37.0","text":"<p>Breaking changes</p> <ul> <li>The API for <code>license_components</code> has been changed, due to the rename of the existing license fields to <code>imported_declared_license_...</code> in https://github.com/MaibornWolff/SecObserve/pull/3229.</li> </ul>"},{"location":"getting_started/upgrading/#release-1300","title":"Release 1.30.0","text":"<p>Noteable change</p> <ul> <li>If multiple licenses have been found for a component, they are now evaluated like an <code>AND</code> expression. If for example one license is <code>Allowed</code> and the other one is <code>Forbidden</code>, the component is evaluated as <code>Forbidden</code>. An explicit rule in a License Policy is not necessary anymore. This new behaviour comes into effect with the next import of components.</li> <li>There is now an explicit menu in the UI and an API endpoint to import SBOMs to get all components with their licenses and dependencies, see Upload SBOMs.</li> </ul>"},{"location":"getting_started/upgrading/#release-1260","title":"Release 1.26.0","text":"<p>Breaking changes</p> <ul> <li>The attribute <code>unknown_license</code> in License Components and License Policies has been renamed to <code>non_spdx_license</code>. This was necessary to avoid confusion with the License Policy evaluation result <code>Unknown</code>, when a license is not included in the License Policy.</li> <li>Additionally the attributes <code>name</code>, <code>version</code>, <code>name_version</code>, <code>dependencies</code>, <code>purl</code>, <code>purl_type</code> and <code>cpe</code> in License Components have been renamed to <code>component_name</code>, <code>component_version</code>, <code>component_name_version</code>, <code>component_dependencies</code>, <code>component_purl</code>, <code>component_purl_type</code> and <code>component_cpe</code> respectively. This brings it more in line with the component information in Observations.</li> </ul> <p>Noteable change</p> <ul> <li>The parser does not need to specified anymore when importing observations from files via the API or the UI. The parser is detected automatically by the content of the imported file. If the parser is still in the attributes of the API call, it will be ignored.</li> </ul>"},{"location":"getting_started/upgrading/#release-1220","title":"Release 1.22.0","text":"<p>Breaking changes</p> <ul> <li>Due to a library change, the <code>OCSF (Open Cybersecurity Schema Framework)</code> parser now only supports reports from Prowler 4.5.0 and above.</li> </ul>"},{"location":"getting_started/upgrading/#release-1210","title":"Release 1.21.0","text":"<p>Breaking changes</p> <ul> <li>There was a typo in severities, where there was a missing \"n\" in \"Unknown\". This has been fixed in the code as well as in the data. If you use the severity \"Unknown\" in API calls, you need to change it from \"Unkown\" to \"Unknown\".</li> </ul>"},{"location":"getting_started/upgrading/#release-1180","title":"Release 1.18.0","text":"<p>Breaking changes</p> <ul> <li>The <code>Prowler</code> parser has been renamed to <code>Prowler 3</code>, because it supports only Prowler up to version 3. For Prowler version 4 and above use the <code>OCSF (Open Cybersecurity Schema Framework)</code> parser.</li> <li>Component dependencies are now shown as a diagram. To do this, the format of the dependencies in the database had to be changed. The migration to the new format is not completely lossless and might loose some information. With the next import of observations, the dependencies will be complete again.</li> </ul>"},{"location":"getting_started/upgrading/#release-150","title":"Release 1.5.0","text":"<p>Breaking changes</p> <ul> <li>The tag of the docker image is not part of the identity hash anymore, to allow updates of the docker image within a vulnerability check without creating a new observation.</li> </ul>"},{"location":"getting_started/upgrading/#release-130","title":"Release 1.3.0","text":"<p>Breaking changes</p> <ul> <li>The ZAP project has had a rebranding as a result of the move to the Software Security Project. To reflect this, the name of the parser has been changed from <code>OWASP ZAP</code> to <code>ZAP</code>. The GitLab template and GitLab action for <code>ZAP</code> have been renamed as well. These changes are not backwards compatible, so you need to update your configuration files and pipelines.</li> </ul>"},{"location":"getting_started/upgrading/#release-110","title":"Release 1.1.0","text":"<p>Breaking changes</p> <ul> <li>When OIDC authentication is used, the environment variable <code>OIDC_CLIENT_ID</code> needs to be set for the backend as well. See Configuration and OpenID Connect authentication for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-100","title":"Release 1.0.0","text":"<p>Breaking changes</p> <ul> <li>SecObserve now supports different OpenID Connect providers for authentication and the Microsoft specific dependencies have been removed. Thus the <code>AAD_</code> configuration parameters are not valid anymore and have been replaced with <code>OIDC_</code> parameters, see Configuration and OpenID Connect authentication for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-099","title":"Release 0.9.9","text":"<p>Breaking changes</p> <ul> <li>The value of the configuration parameter <code>MYSQL_AZURE</code> has been changed from <code>true</code> to <code>flexible</code> or <code>single</code>, depending on the type of Azure Database for MySQL. See Configuration for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-091","title":"Release 0.9.1","text":"<p>Breaking changes</p> <ul> <li> <p>The SSLyze parser has been replaced by the CryptoLyzer parser due to licensing reasons. Even though the SSLyze parser may still be seen in the list of parsers, you cannot use it for imports anymore. The CryptoLyter parser generates the same kind of results, adding information about signature algorithms.</p> </li> <li> <p>The project name <code>secobserve_prod</code> has been set in <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code>. This was necessary to assign defined network names, but it changes the name of the database volume. You need to dump the database content to a file before the upgrade and restore it after the upgrade.</p> </li> </ul>"},{"location":"integrations/api_import/","title":"API import","text":""},{"location":"integrations/api_import/#api-configuration","title":"API configuration","text":""},{"location":"integrations/api_import/#dependency-track","title":"Dependency-Track","text":"<p>The Dependency-Track API configuration requires the URL of the Dependency-Track instance, an API key and a project key:</p> <ul> <li>The base URL of the Dependency-Track instance is something like <code>https://dependencytrack.example.com</code></li> <li>The API key can be created for a team in the Access Management section of the Dependency-Track administration</li> <li>The project key is the identifier of the project in Dependency-Track. It can be found as the last part of the URL of the project page in the form of a UUID.</li> </ul>"},{"location":"integrations/api_import/#trivy-operator-prometheus","title":"Trivy Operator Prometheus","text":"<p>The Trivy Operator must be installed with the Prometheus integration enabled as described in the Trivy Operator documentation.</p> <p>The Trivy Operator Prometheus API configuration requires the URL of the Prometheus instance, a query and optionally a basic authentication:</p> <ul> <li>The base URL of the Dependency-Track instance is something like <code>https://prometheus.example.com</code></li> <li> <p>The query is a Prometheus query that returns the desired data. Currently there a these options:</p> <ul> <li><code>trivy_compliance_info</code>: Compliance information of the whole cluster related to CIS and NSA/CISA guidelines</li> <li><code>trivy_configaudits_info</code>: Misconfigurations of the cluster</li> <li><code>trivy_exposedsecrets_info</code>: Exposed secrets in deployed Docker images</li> <li><code>trivy_rbacassessments_info</code>: Failed assessment results for the role-based access control (RBAC) of the cluster</li> <li><code>trivy_vulnerability_id</code>: Known vulnerabilities in deployed Docker images</li> </ul> </li> <li> <p>If the Prometheus instance is protected by basic authentication, the username and password can be entered after turning <code>Basic authentication</code> on.</p> </li> </ul>"},{"location":"integrations/api_import/#automatic-import","title":"Automatic import","text":"<p>API imports can be configured to run automatically at a specific time. There is a general setting and a setting per API configuration.</p>"},{"location":"integrations/api_import/#general-setting","title":"General setting","text":"<p>In the <code>Features</code> section of the Settings the automatic import can be enabled for the whole SecObserve instance.</p> <p></p> <p>The hour (in UTC time) and minute, when the automatic API imports and the OSV scanning will run, can be set in the <code>Background tasks</code>  section. A restart of the SecObserve instance is required to apply the changes.</p> <p></p>"},{"location":"integrations/api_import/#setting-per-api-configuration","title":"Setting per API configuration","text":"<p>Only API configurations that have <code>Automatic import enabled</code> turned on will be imported automatically. When the automatic import is enabled for an API configuration, the values for <code>Branch / Version</code>, <code>Service</code>, <code>Docker image name:tag</code>, <code>Endpoint URL</code>and <code>Kubernetes cluster</code> can be set to be used for the import.</p> <p></p>"},{"location":"integrations/codecharta/","title":"CodeCharta","text":"<p>CodeCharta is a tool to visualize software quality. It allows you to import and combine metrics from various sources like SonarQube, Svn, Git or directly from your source code. The visualization makes the quality of a code base tangible and can be used to gain insights and communicate with stakeholders.</p> <p></p>"},{"location":"integrations/codecharta/#export-data-from-secobserve","title":"Export data from SecObserve","text":"<p>When showing a product, there is an <code>Export</code> button. When clicking it, it shows a menu including an option to export the metrics about observations of this product to CodeCharta:</p> <p></p> <p>This export produces a CSV file with severity metrics for all open observations that have a source file as an origin.</p>"},{"location":"integrations/codecharta/#process-data-for-codecharta","title":"Process data for CodeCharta","text":"<p>The CSV file needs to be converted to CodeCharta's JSON format. First the analysis tools of CodeCharta need to be installed. Then the exported metrics are converted like this:</p> <pre><code>ccsh csvimport secobserve_codecharta_metrics.csv -o secobserve_codecharta_metrics.cc.json\n</code></pre> <p>This produces the file <code>secobserve_codecharta_metrics.cc.json.gz</code>. This file can be visualized by CodeCharta, but contains only the source files with vulnerabilities.</p> <p>To get a better picture of the whole system, this file can be combined with an export from SonarQube or the Source Code Parser.</p> <p>When SonarQube is used, the export from SonarQube will include an additional node with the id of the project, that needs to be removed:</p> <pre><code>ccsh modify -f root/csec_secobserve -t root -o secobserve_sonarqube_modified.cc.json.gz secobserve_sonarqube.cc.json.gz\n</code></pre> <p>Now the results from SonarQube and SecObserve can be merged:</p> <pre><code>ccsh merge secobserve_sonarqube_modified.cc.json secobserve_codecharta_metrics.cc.json.gz -o secobserve.cc.json.gz\n</code></pre>"},{"location":"integrations/codecharta/#visualize-data-in-codecharta","title":"Visualize data in CodeCharta","text":"<p>The resulting file <code>secobserve.cc.json.gz</code> can now be visualized using https://maibornwolff.github.io/codecharta/visualization/app/index.html. These SecObserve metrics are included:</p> <ul> <li>vulnerabilities_total</li> <li>vulnerabilities_critical</li> <li>vulnerabilities_high</li> <li>vulnerabilities_medium</li> <li>vulnerabilities_low</li> <li>vulnerabilities_none</li> <li>vulnerabilities_unknown</li> <li>vulnerabilities_high_and_above</li> <li>vulnerabilities_medium_and_above</li> <li>vulnerabilities_low_and_above</li> </ul>"},{"location":"integrations/epss/","title":"Exploit Prediction Scoring System (EPSS)","text":"<p>The Exploit Prediction Scoring System (EPSS) is a data-driven effort for estimating the likelihood (probability) that a software vulnerability will be exploited in the wild. The EPSS model produces a probability score between 0 and 1 (0 and 100%) for all CVE vulnerabilities. The higher the score, the greater the probability that a vulnerability will be exploited. Additionally percentiles are calculated, which are a direct transformation from probabilities and provide a measure of an EPSS probability relative to all other scores. That is, the percentile is the proportion of all values less than or equal to the current rank. A good overview of EPSS scores and EPSS percentiles is given in Probability, Percentiles, and Binning - How to understand and interpret EPSS Scores. The EPSS data is updated daily.</p> <p>SecObserve imports the EPSS data and updates all observations with a CVE value with the EPSS score and EPSS percentile regularly. After an import of a vulnerability scan, all observations with a CVE number contained in the import are updated with the EPSS score and EPSS percentile as well.</p>"},{"location":"integrations/epss/#configuration","title":"Configuration","text":"<p>Per default the task to import the EPSS data and update the observations is scheduled to run every night at 03:00 UTC time. This default can be changed by administrators via the Background tasks section in the Settings.  Hours are always in UTC time.</p> <p></p>"},{"location":"integrations/exploit_information/","title":"Exploit information","text":"<p>Several databases and tools collect information about exploits of known vulnerabilities. This information is important to prioritize the remediation of vulnerabilities. The project cvss-bt collects information about exploits from various sources which can be imported automatically into SecObserve. The exploit information is linked to the corresponding observations via the CVE Id.</p> <p>Exploit information from these sources is made available:</p> <ul> <li>CISA Known Exploited Vulnerabilities Catalog</li> <li>Exploit-DB</li> <li>Metasploit</li> <li>Nuclei</li> <li>PoC GitHub</li> <li>VulnCheck KEV</li> </ul> <p>Observations can be filtered by the presence of exploit information and the links to exploit information are displayed in the observation details.</p>"},{"location":"integrations/exploit_information/#configuration","title":"Configuration","text":"<p>In the <code>Features</code> section of the Settings the import of exploit information can be enabled or disabled for the whole SecObserve instance. Additionally, the maximum age of CVEs can be set. Data for CVEs older than the specified number of days will not be imported.</p> <p></p> <p>The import of exploit information runs together with the EPSS import, see EPSS import / Configuration.</p>"},{"location":"integrations/github_actions_and_templates/","title":"GitHub actions and GitLab CI templates","text":"<p>Integrating vulnerability scanners in a CI/CD pipeline can be cumbersome. Every tool is different to install and has different parameters. Our repository of GitHub actions and GitLab CI templates makes this process very straightforward, with a unified way to start the tools. The tools in the template repository will be updated regularly, so that all the latest features and bugfixes are available.</p> <p>All actions and templates run the scanner, import the results into SecObserve and make the report available as an artifact.</p> <p>The actions and the templates are stored in the repository https://github.com/MaibornWolff/secobserve_actions_templates.</p>"},{"location":"integrations/github_actions_and_templates/#variables","title":"Variables","text":"<p>Most of the actions and templates use the same set of variables:</p> Variable Optionality Description Scanning <code>TARGET</code> mandatory The target to be scanned, often it is a path of the filesystem, but can be a Docker image, an URL or others. <code>REPORT_NAME</code> mandatory The name of the report to be written. It will be saved as an artifact. <code>RUN_DIRECTORY</code> optional The directory where to run the scanner, only to be used when the <code>TARGET</code> is a path. <code>FURTHER_PARAMETERS</code> optional Further parameters to be given to the scanner. <code>CONFIGURATION</code> mandatory, only for Semgrep Configuration to be used with Semgrep. <code>OUTPUT_PATH</code> optional, only for KICS Path to the output file, default is <code>.</code>. <code>RULES</code> optional, only for DrHeader Custom rules to be used with DrHeader. <code>SCRIPT</code> optional, only for ZAP Script to be executed, default is <code>zap-baseline.py</code>. Importing <code>SO_UPLOAD</code> optional No upload of observations into SecObserve if value is not <code>true</code>, default is <code>true</code>. <code>SO_API_BASE_URL</code> mandatory Base URL of the SecObserve backend, e.g. <code>https://secobserve-backend.example.com</code>. <code>SO_API_TOKEN</code> mandatory API token of the user to be used for the import. The users needs at least the <code>Upload</code> role. <code>SO_PRODUCT_NAME</code> mandatory Name of the product which observations are imported. The product has to exist before starting the import. <code>SO_BRANCH_NAME</code> optional Name of the branch in the source code repository. <code>SO_ORIGIN_SERVICE</code> optional Service name to be set for all imported observations. <code>SO_ORIGIN_DOCKER_IMAGE_NAME_TAG</code> optional Name:Tag of Docker image to be set for all imported observations. <code>SO_ORIGIN_ENDPOINT_URL</code> optional URL of endpoint to be set for all imported observations. <code>SO_SUPPRESS_LICENSES</code> optional, only for CycloneDX Suppress importing license information if value is <code>true</code>. Default is <code>true</code> for the Grype, Trivy Filesystem and Trivy Image GitHub action / GitLab templates, default is <code>false</code> for the Importer  action/template Check security gate <code>SO_API_BASE_URL</code> mandatory Base URL of the SecObserve backend, e.g. <code>https://secobserve-backend.example.com</code>. <code>SO_API_TOKEN</code> mandatory API token of the user to be used for the check. <code>SO_PRODUCT_NAME</code> mandatory Name of the product for which the security gate check is being performed."},{"location":"integrations/github_actions_and_templates/#available-actions-and-templates","title":"Available actions and templates","text":"Scanner GitHub Action GitLab CI Template License SCA Grype <code>actions/SCA/grype_image</code> <code>templates/SCA/grype_image.yml</code> Apache 2.0 Trivy <code>actions/SCA/trivy_filesystem</code> <code>templates/SCA/trivy_filesystem.yml</code> Apache 2.0 Trivy <code>actions/SCA/trivy_image</code> <code>templates/SCA/trivy_image.yml</code> Apache 2.0 SAST application Bandit <code>actions/SAST/bandit</code> <code>templates/SAST/bandit.yml</code> Apache 2.0 ESLint <code>actions/SAST/eslint</code> <code>templates/SAST/eslint.yml</code> MIT Semgrep <code>actions/SAST/semgrep</code> <code>templates/SAST/semgrep.yml</code> LGPL 2.1 SAST infrastructure Checkov <code>actions/SAST/checkov</code> <code>templates/SAST/checkov.yml</code> Apache 2.0 KICS <code>actions/SAST/kics</code> <code>templates/SAST/kics.yml</code> Apache 2.0 tfsec <code>actions/SAST/tfsec</code> <code>templates/SAST/tfsec.yml</code> MIT Trivy <code>actions/SAST/trivy_config</code> <code>templates/SAST/trivy_config.yml</code> Apache 2.0 Secrets Gitleaks <code>actions/secrets/gitleaks</code> <code>templates/secrets/gitleaks.yml</code> MIT Trivy <code>actions/secrets/trivy_filesystem_secrets</code> <code>templates/secrets/trivy_filesystem_secrets.yml</code> Apache 2.0 Trivy <code>actions/secrets/trivy_image_secrets</code> <code>templates/secrets/trivy_image_secrets.yml</code> Apache 2.0 DAST CryptoLyzer <code>actions/DAST/cryptolyzer</code> <code>templates/DAST/cryptolyzer.yml</code> MPL 2.0 DrHeader <code>actions/DAST/drheader</code> <code>templates/DAST/drheader.yml</code> MIT ZAP <code>actions/DAST/zap</code> <code>templates/DAST/zap.yml</code> Apache 2.0 Task GitHub Action GitLab CI Template Import existing file into SecObserve <code>actions/importer</code> <code>templates/importer.yml</code> Check security gate of a product (<code>exit code 1</code> if security gate Failed, <code>exit code 0</code> if security gate Passed or Disabled) <code>actions/check_security_gate</code> <code>templates/check_security_gate.yml</code> Upload SBOM into SecObserve <code>actions/upload_sbom</code> <code>templates/upload_sbom.yml</code> <p>All GitHub actions and GitLab CI templates use a pre-built Docker image that contains all scanners and the SecObserve importer.</p>"},{"location":"integrations/github_actions_and_templates/#examplary-workflow-for-github-actions","title":"Examplary workflow for GitHub actions","text":"<p>Tip</p> <p>The mandatory variables for importing (<code>SO_API_BASE_URL</code>, <code>SO_API_TOKEN</code>, <code>SO_PRODUCT_NAME</code>) can be set as secrets and variables in the settings of the project in GitHub.</p> <pre><code>name: Vulnerability checks\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Run Bandit\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/bandit@main\n        with:\n          target: 'backend'\n          report_name: 'backend_bandit.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Semgrep\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/semgrep@main\n        with:\n          target: 'backend'\n          report_name: 'backend_semgrep.json'\n          configuration: 'r/python'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run KICS\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/kics@main\n        with:\n          target: '.'\n          report_name: 'backend_kics.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Checkov\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/checkov@main\n        with:\n          target: '.'\n          report_name: 'backend_checkov.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Trivy image\n        uses: MaibornWolff/secobserve_actions_templates/actions/SCA/trivy_image@main\n        with:\n          target: 'maibornwolff/secobserve-backend:latest'\n          report_name: 'backend_trivy_image.json'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Grype image\n        uses: MaibornWolff/secobserve_actions_templates/actions/SCA/grype_image@main\n        with:\n          target: 'maibornwolff/secobserve-backend:latest'\n          report_name: 'backend_grype_image.json'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Gitleaks\n        uses: MaibornWolff/secobserve_actions_templates/actions/secrets/gitleaks@main\n        with:\n          report_name: 'backend_gitleaks.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Upload results\n        uses: actions/upload-artifact@v3\n        with:\n          name: secobserve\n          path: |\n            backend_bandit.sarif\n            backend_semgrep.json\n            backend_kics.sarif\n            backend_checkov.sarif\n            backend_trivy_image.json\n            backend_grype_image.json\n            backend_gitleaks.sarif\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#examplary-pipeline-for-gitlab-ci-templates","title":"Examplary pipeline for GitLab CI templates","text":"<p>Tip</p> <p>The mandatory variables for importing (<code>SO_API_BASE_URL</code>, <code>SO_API_TOKEN</code> and <code>SO_PRODUCT_NAME</code>) can be set as variables in the CI/CD settings of the project in GitLab. Then they don't need to be set in each job.</p> <pre><code>include:\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/DAST/drheader.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/DAST/cryptolyzer.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/bandit.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/checkov.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/eslint.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/kics.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/semgrep.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/grype_image.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/trivy_filesystem.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/trivy_image.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/secrets/gitleaks.yml\"\n\ngrype_image_backend:\n  extends: .grype_image\n  variables:\n    TARGET: \"$BACKEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"grype_backend_image.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n\ngrype_image_frontend:\n  extends: .grype_image\n  variables:\n    TARGET: \"$FRONTEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"grype_frontend_image.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n\ntrivy_image_backend:\n  extends: .trivy_image\n  variables:\n    TARGET: \"$BACKEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"trivy_backend_image.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n\ntrivy_image_frontend:\n  extends: .trivy_image\n  variables:\n    TARGET: \"$FRONTEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"trivy_frontend_image.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n\ntrivy_filesystem_frontend:\n  extends: .trivy_filesystem\n  variables:\n    TARGET: \"frontend/package-lock.json\"\n    REPORT_NAME: \"trivy_frontend_npm.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\nbandit_backend:\n  extends: .bandit\n  variables:\n    TARGET: \"backend\"\n    REPORT_NAME: \"bandit_backend.sarif\"\n    SO_ORIGIN_SERVICE: \"backend\"\n  needs: []\n\neslint_frontend:\n  extends: .eslint\n  variables:\n    RUN_DIRECTORY: \"frontend\"\n    TARGET: \"src\"\n    REPORT_NAME: \"eslint_frontend.sarif\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\nsemgrep_backend:\n  extends: .semgrep\n  variables:\n    CONFIGURATION: \"r/python\"\n    TARGET: \"backend\"\n    REPORT_NAME: \"semgrep_backend.sarif.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n  needs: []\n\nsemgrep_frontend:\n  extends: .semgrep\n  variables:\n    CONFIGURATION: \"r/typescript\"\n    TARGET: \"frontend/src\"\n    REPORT_NAME: \"semgrep_frontend.sarif.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\ngitleaks:\n  extends: .gitleaks\n  variables:\n    REPORT_NAME: \"gitleaks.sarif\"\n  needs: []\n\n\ncheckov:\n  extends: .checkov\n  variables:\n    TARGET: \".\"\n    REPORT_NAME: \"checkov.sarif\"\n  needs: []\n\nkics:\n  extends: .kics\n  variables:\n    TARGET: \".\"\n    REPORT_NAME: \"kics.sarif\"\n  needs: []\n\ndrheader:\n  extends: .drheader\n  variables:\n    TARGET: \"https://secobserve.example.com\"\n    REPORT_NAME: \"drheader.json\"\n    SO_ORIGIN_ENDPOINT_URL: \"https://secobserve.example.com\"\nneeds: []\n\ncryptolyzer:\n  extends: .cryptolyzer\n  variables:\n    TARGET: \"secobserve.example.com\"\n    REPORT_NAME: \"cryptolyzer.json\"\n  needs: []\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#using-a-configuration-file","title":"Using a configuration file","text":"<p>Using multiple vulnerability scanners makes the pipeline quite complex. To make the pipeline smaller, a configuration file can be used to define the scanners to be used with their parameters. The configuration file is a YAML file with sections per scanner and one section for the import into SecObserve.</p>"},{"location":"integrations/github_actions_and_templates/#example-pipeline-for-github","title":"Example pipeline for GitHub","text":"<pre><code>name: Check for vulnerabilities in the code\n\non: [push]\n\npermissions: read-all\n\njobs:\n  check_vulnerabilities:\n\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Run vulnerability scanners\n        uses: MaibornWolff/secobserve_actions_templates/actions/vulnerability_scanner@main\n        with:\n          so_configuration: 'so_configuration.yml'\n          SO_API_TOKEN: ${{ secrets.SO_API_TOKEN }}\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#example-pipeline-for-gitlab","title":"Example pipeline for GitLab","text":"<pre><code>include:\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/vulnerability_scanner.yml\"\n\nvulnerability_scans:\n  stage: test\n  extends: .vulnerability_scanner\n  variables:\n    SO_CONFIGURATION: \"so_configuration.yml\"\n  needs: []\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#example-configuration-file","title":"Example configuration file","text":"<pre><code>bandit_backend:\n  SCANNER: bandit\n  RUN_DIRECTORY: \".\"\n  TARGET: backend\n  REPORT_NAME: bandit_backend.sarif\n  SO_ORIGIN_SERVICE: backend\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ncheckov:\n  SCANNER: checkov\n  RUN_DIRECTORY: \".\"\n  TARGET: \".\"\n  REPORT_NAME: checkov.sarif\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\neslint_frontend:\n  SCANNER: eslint\n  RUN_DIRECTORY: \"frontend\"\n  TARGET: \"src\"\n  REPORT_NAME: \"eslint_frontend.sarif\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ngitleaks:\n  SCANNER: gitleaks\n  RUN_DIRECTORY: \".\"\n  REPORT_NAME: \"gitleaks.sarif\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nkics:\n  SCANNER: kics\n  RUN_DIRECTORY: \".\"\n  TARGET: \".\"\n  REPORT_NAME: \"kics.sarif\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nsemgrep_backend:\n  SCANNER: semgrep\n  RUN_DIRECTORY: \".\"\n  CONFIGURATION: \"r/python\"\n  TARGET: \"backend\"\n  REPORT_NAME: \"semgrep_backend.json\"\n  SO_ORIGIN_SERVICE: \"backend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nsemgrep_frontend:\n  SCANNER: semgrep\n  RUN_DIRECTORY: \".\"\n  CONFIGURATION: \"r/typescript\"\n  TARGET: \"frontend/src\"\n  REPORT_NAME: \"semgrep_frontend.json\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ntrivy_filesystem_frontend:\n  SCANNER: trivy_filesystem\n  RUN_DIRECTORY: \".\"\n  TARGET: \"frontend/package-lock.json\"\n  REPORT_NAME: \"trivy_frontend_npm.json\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nimporter:\n  SO_UPLOAD: \"true\"\n  SO_API_BASE_URL: https://secobserve-backend.example.com\n  SO_PRODUCT_NAME: SecObserve\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#real-life-examples","title":"Real life examples","text":"<p>Some real life examples can be found in the SecObserve GitHub repository:</p> <ul> <li>so_configuration_code.yml used in pipeline check_vulnerabilities.yml</li> <li>so_configuration_sca_dev.yml used in pipeline build_push_dev.yml</li> <li>so_configuration_endpoints.yml used in pipeline build_push_release.yml</li> </ul>"},{"location":"integrations/issue_trackers/","title":"Issue trackers","text":"<p>Developers and product owners will typically document the development tasks in an issue tracker and move these issues in a Scrum or Kanban board. Therefore, even though the assessment of observations is done in SecObserve, it may be useful to transfer them to an issue tracker as well. SecObserve currently supports automatic creation of issues in GitHub, GitLab and Jira (Cloud). </p> <p>The synchronization of issues is an unidirectional process:</p> <ul> <li>A new observation will be pushed as an open issue, if its branch / version is set to the default branch / version of the product.</li> <li>If the observation changes its priority or description, the issue will be updated accordingly.</li> <li>If the observation changes the status from Open to another status by an assessment in SecObserve, the issue will be closed.</li> <li>If the observation gets the status Resolved in SecObserve because it is not found in the results anymore, the issue will be closed as well.</li> <li>If the observation is reopened in SecObserve by an assessment or because it has been found in the results again, the issue will be opened again.</li> <li>If the observation is deleted in SecObserve, the issue will be closed.</li> </ul> <p>If the issue is closed or deleted in the issue tracker, it will be reopened or recreated with the next import of results in SecObserve, when the respective observation was still found and still has the status Open in SecObserve.</p> <p>The parameters for the issue tracker integration are set in the product:</p> <p></p> Active Issues will only be pushed, if this parameter is set. Type Either GitHub or GitLab or Jira Base URL The base URL of the issue tracker. For GitHub it is <code>https://api.github.com</code>, for a self hosted GitLab it will be something like <code>https://gitlab.example.com</code>, for Jira it is <code>https:\\\\{organization_name}.atlassian.net</code>. API key An API key must be created in the issue tracker, having the permissions to create and update issues. Project id The path of the repository in its URL in GitHub or GitLab, e.g. <code>MaibornWolff/SecObserve</code>. For Jira it is the key of the project. Labels A comma separated list of labels, that will be set for the issue. Additional labels can be set in the issue tracker, they will be preserved when the issue is updated. Minimum severity (optional) Issues will only be exported for observations with a severity that is higher or the same. Username (only for Jira) The REST API of Jira needs an authentication with username and API key. Issue type (only for Jira) The issue type to be created. Closed status (only for Jira) The status to be set when an issue is closed. <p>Issues are created or updated by an asynchronous background process after the import or the assessment of an observation has finished. If problems should occur during the transfer, a notification is send, see Notifications.</p> <p>Tip</p> <p>Issues shouldn't be created when observations are imported the first time for a vulnerability scannner. First the number of observations should be minimized with settings of the vulnerability scanner of rules within SecObserve, before pushing issues to an issue tracker.</p>"},{"location":"integrations/issue_trackers/#issue-in-github","title":"Issue in GitHub","text":""},{"location":"integrations/issue_trackers/#issue-in-gitlab","title":"Issue in GitLab","text":""},{"location":"integrations/issue_trackers/#issue-in-jira-cloud","title":"Issue in Jira (Cloud)","text":""},{"location":"integrations/license_data/","title":"License data import","text":""},{"location":"integrations/license_data/#spdx-licenses","title":"SPDX licenses","text":"<p>The Linux Foundation gathers a list of commonly found licenses and exceptions used for open source and other collaborative software. The list is called SPDX License List and is imported daily into SecObserve from a JSON file hosted on GitHub.</p>"},{"location":"integrations/license_data/#configuration","title":"Configuration","text":"<p>Per default the task to import the SPDX licenses is scheduled to run every night at 01:30 UTC time. This default can be changed by administrators via the Settings. </p> <p></p> <p>Hour is always in UTC time.</p>"},{"location":"integrations/license_data/#scancode-licensedb","title":"ScanCode LicenseDB","text":"<p>The ScanCode LicenseDB is a free and open database of mostly all the software licenses, including a category per license. Administrators can import the data from the ScanCode LicenseDB with a button in the list of License Groups. License groups will be created or updated for each category, containing the respective SPDX licenses.</p>"},{"location":"integrations/links/","title":"Links to additional information","text":""},{"location":"integrations/links/#references","title":"References","text":"<p>Most of the vulnerability scanners include references to further information about the vulnerabilities. These references are imported with the observation and can be accessed by clicking on the link icon in the <code>References</code> box, on the right side of the Observation view.</p> <p></p>"},{"location":"integrations/links/#vulnerabilities","title":"Vulnerabilities","text":"<p>If an observation is a vulnerability with a CVE or GHSA number, the Vulnerability ID in the <code>Vulnerability</code> box will be a link to the National Vulnerabilities Database (NVD) or the GitHub Advisory Database.</p> <p></p>"},{"location":"integrations/links/#components","title":"Components","text":"<p>If an observation has a component with a PURL as its origin, a link to further information about the package can be shown. The link can go either to open/source/insights or ecosyste.ms, depending on the user settings.</p> <p></p>"},{"location":"integrations/links/#opensourceinsights","title":"open/source/insights","text":"<p>If the preference in the user settings has been set to <code>deps.dev</code> and the package type is in</p> <ul> <li><code>cargo</code> (Rust)</li> <li><code>go</code> (Go)</li> <li><code>maven</code> (Java)</li> <li><code>npm</code> (JavaScript / TypeScript)</li> <li><code>nuget</code> (.NET)</li> <li><code>pypi</code> (Python)</li> </ul> <p>the Component PURL in the <code>Origins</code> box will be a link to the open/source/insights platform, which provides insights into the open source component containing the vulnerability. It helps you to understand the security, licensing, and maintenance aspects of the component.</p> <p></p>"},{"location":"integrations/links/#ecosystems","title":"ecosyste.ms","text":"<p>If the preference in the user settings has been set to <code>ecosyste.ms</code> and the package type is in</p> <ul> <li><code>cargo</code> (Rust)</li> <li><code>cocoapods</code> (iOS / macOS)</li> <li><code>composer</code> (PHP)</li> <li><code>cpan</code> (Perl)</li> <li><code>cran</code> (R)</li> <li><code>gem</code> (Ruby)</li> <li><code>golang</code> (Go)</li> <li><code>hackage</code> (Haskell)</li> <li><code>maven</code> (Java)</li> <li><code>npm</code> (JavaScript / TypeScript)</li> <li><code>nuget</code> (.NET)</li> <li><code>pypi</code> (Python)</li> </ul> <p>the Component PURL in the <code>Origins</code> box will be a link to the ecosyste.ms platform.</p> <p></p>"},{"location":"integrations/notifications/","title":"Notifications","text":"<p>SecObserve can send notifications to email addresses, Microsoft Teams or Slack for 3 kinds of events:</p> <ul> <li>When the security gate of a product changes.</li> <li>When an exception occurs while processing a request.</li> <li>When an exception occurs in a background task.</li> </ul> <p>There is a ratelimiting active to prevent flooding of notifications, if a series of exceptions occurs. The same exception is sent only once during a specified timedelta, which can be configured in the Settings. The default for this timedelta is 1 hour.</p>"},{"location":"integrations/notifications/#notifications-to-email-addresses","title":"Notifications to email addresses","text":""},{"location":"integrations/notifications/#settings-in-secobserve","title":"Settings in SecObserve","text":"<p>The field <code>EMAIL_FROM</code> needs to be set in the Settings to be able to send notifications to email addresses for both events. </p>"},{"location":"integrations/notifications/#notifications-for-security-gates","title":"Notifications for security gates","text":"<p>When creating or editing a product, the field <code>Email</code> can be set in the Notification section with a comma separated list of email addresses. If the security gate of the product changes and this field is filled, then a notification is sent each of the email addresses.</p> <p></p>"},{"location":"integrations/notifications/#notifications-for-exceptions","title":"Notifications for exceptions","text":"<p>An admistrator can configure the field <code>EXCEPTION_EMAIL_TO</code> in the Settings. If an exception occurs while processing a request and this field is filled with a comma separated list of email addresses, a notifications is sent each of the email addresses before returning the HTTP code 500 via the REST API.</p>"},{"location":"integrations/notifications/#notifications-to-microsoft-teams-and-slack","title":"Notifications to Microsoft Teams and Slack","text":""},{"location":"integrations/notifications/#settings-in-microsoft-teams","title":"Settings in Microsoft Teams","text":"<p>For both types of notifications an incoming webhook has to be set for a channel, where the notifications shall appear. How to do this is explained in Create Incoming Webhooks. Copy the URL of the webhook to the clipboard, to have it available to set it in SecObserve.</p> <p>The messages do not include mentions, but a user can set the \"Channel notifications\" to \"All activities\" in Teams, to get an active notification when an entry is generated. </p>"},{"location":"integrations/notifications/#settings-in-slack","title":"Settings in Slack","text":"<p>For both types of notifications an incoming webhook has to be set for a channel, where the notifications shall appear. How to do this is explained in Sending messages using Incoming Webhooks. Copy the URL of the webhook to the clipboard, to have it available to set it in SecObserve.</p>"},{"location":"integrations/notifications/#notifications-for-security-gates_1","title":"Notifications for security gates","text":"<p>When creating or editing a product, the fields <code>MS Teams</code> and/or <code>Slack</code> can be set in the Notification section with the copied webhook URL. If the security gate of the product changes and this field is filled, then a notification is sent to Microsoft Teams and/or Slack.</p> <p></p>"},{"location":"integrations/notifications/#notifications-for-exceptions_1","title":"Notifications for exceptions","text":"<p>An admistrator can configure the fields <code>EXCEPTION_MS_TEAMS_WEBHOOK</code> and/or <code>EXCEPTION_SLACK_WEBHOOK</code> in the Settings. If an exception occurs while processing a request and this field is filled with the copied webhook URL, a notifications is sent to Microsoft Teams and/or Slack before returning the HTTP code 500 via the REST API.</p>"},{"location":"integrations/notifications/#notifications-in-the-user-interface","title":"Notifications in the user interface","text":"<p>Notifications are also stored in the database and can be viewed in the user interface.</p> <ul> <li>Regular users can view notifications for changed security gates and exceptions in background tasks for all products where they are a product member.</li> <li>Administrators can view all notifications.</li> </ul> <p></p> <p>When a notification is deleted, it is removed from the database and won't be visible anymore for all users.</p>"},{"location":"integrations/observations_export/","title":"Export of observations","text":"<p>Observations of a product can be exported to Excel or CSV. When showing a product, there is an <code>Export</code> button. When clicking it, it shows a menu with several options to export the observations of this product:</p> <p></p>"},{"location":"integrations/oidc_authentication/","title":"OpenID Connect authentication","text":"<p>OpenID Connect authentication has been tested with Keycloak and Azure Active Directory. It should work with other OpenID Connect providers as well, as long as they support the authorization flow with PKCE and without a secret.</p>"},{"location":"integrations/oidc_authentication/#keycloak","title":"Keycloak","text":"<p>In Keycloak a new OpenID Connect client needs to be created. The client needs to be configured as follows, assuming the frontend is available at <code>https://secobserve.example.com</code>:</p> <p></p>"},{"location":"integrations/oidc_authentication/#configuration-parameters-for-secobserve","title":"Configuration parameters for SecObserve","text":"<p>Backend</p> Environment variable Value <code>OIDC_AUTHORITY</code> <code>https://keycloak.example.com/realms/NAME_OF_REALM</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_USERNAME</code> <code>preferred_username</code> <code>OIDC_FIRST_NAME</code> <code>given_name</code> <code>OIDC_LAST_NAME</code> <code>family_name</code> <code>OIDC_EMAIL</code> <code>email</code> <code>OIDC_GROUPS</code> <code>groups</code> <p>Frontend</p> Environment variable Value <code>OIDC_ENABLE</code> <code>true</code> <code>OIDC_AUTHORITY</code> <code>https://keycloak.example.com/realms/NAME_OF_REALM</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_REDIRECT_URI</code> <code>https://secobserve.example.com</code> <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> <code>https://secobserve.example.com</code>"},{"location":"integrations/oidc_authentication/#azure-active-directory","title":"Azure Active Directory","text":"<p>In Azure Active Directory a new App registration needs to be created.</p>"},{"location":"integrations/oidc_authentication/#configuration-parameters-for-secobserve_1","title":"Configuration parameters for SecObserve","text":"<p>Backend</p> Environment variable Value <code>OIDC_AUTHORITY</code> <code>https://login.microsoftonline.com/TENANT_ID/v2.0</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_USERNAME</code> <code>preferred_username</code> <code>OIDC_FULL_NAME</code> <code>name</code> <code>OIDC_EMAIL</code> <code>email</code> <code>OIDC_GROUPS</code> <code>groups</code> <p>Frontend</p> Environment variable Value <code>OIDC_ENABLE</code> <code>true</code> <code>OIDC_AUTHORITY</code> <code>https://login.microsoftonline.com/TENANT_ID</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_REDIRECT_URI</code> <code>https://secobserve.example.com</code> <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> <code>https://secobserve.example.com</code>"},{"location":"integrations/oidc_authentication/#customize-the-login-dialog","title":"Customize the login dialog","text":"<p>If users should only be able to sign in with OpenID Connect, the login dialog can be customized to hide user and password fields. This can be done by setting the <code>Disable user login</code> option in the <code>Settings</code> dialog:</p> <p></p> <p>Then the login dialog will only show the <code>Enterprise sign in</code> button:</p> <p></p> <p>If the user and password is needed to login, e.g. for a local admin user, <code>#force_user_login</code> can be added to the URL (like <code>https://secobserve.example.com/#/login#force_user_login</code>) to force the user and password fields to be shown.</p>"},{"location":"integrations/osv_scan/","title":"Vulnerability scanning from OSV database","text":"<p>The components of a product can be scanned for vulnerabilities using the OSV database. The OSV database is a database of open-source vulnerabilities, maintained by Google and is available at https://osv.dev/.</p> <p>There are 2 preconditions for a product to be met before using the OSV database for vulnerability scanning:</p> <ul> <li>License/Component information has to be imported, only then all components are available for scanning.</li> <li>The flag <code>OSV scanning enabled</code> in the product settings has to be activated.</li> </ul> <p></p> <p>OSV delivers vulnerabilities for a component, without regard to the version of the component, plus a list of affected versions. Currently these package managers are supported directly to get the affected versions, by using information from the PURL of the component:</p> <ul> <li>bitnami: Bitnami</li> <li>conan: ConanCenter</li> <li>cran: CRAN</li> <li>cargo: crates.io</li> <li>golang: Go</li> <li>hackage: Hackage</li> <li>hex: Hex</li> <li>maven: Maven</li> <li>npm: npm</li> <li>nuget: NuGet</li> <li>pub: Pub</li> <li>pypi: PyPI</li> <li>gem: RubyGems</li> <li>swift: SwiftURL</li> </ul> <p>Some Linux distribution and version can be determined automatically by information from the PURL of the component:</p> <ul> <li>Alpine</li> <li>Chainguard</li> <li>Debian</li> <li>Ubuntu (only the free version, not Pro)</li> <li>Wolfi</li> </ul> <p>To correctly identify other Linux packages, the Linux distribution and version has to be set in the product or branch settings according to OSV affected package specification. If it is not set for a branch, the product settings are used. Examples are:</p> Linux distribution Version Red Hat rhel_aus:8.4::appstream Ubuntu Pro:24.04:LTS"},{"location":"integrations/osv_scan/#manual-scan","title":"Manual scan","text":"<p>If both preconditions are met, the OSV scan can be started manually from the <code>Import</code> menu. If a branch is selected, the scan will be performed on the components of the branch. If no branch is selected, the scan will be performed on the components of all branches and components without a branch.</p> <p></p>"},{"location":"integrations/osv_scan/#automatic-scan","title":"Automatic scan","text":"<p>OSV scanning can be configured to run automatically at a specific time. There is a general setting and a setting per API configuration.</p>"},{"location":"integrations/osv_scan/#general-setting","title":"General setting","text":"<p>In the <code>Features</code> section of the Settings the automatic OSV scanning can be enabled or disabled for the whole SecObserve instance.</p> <p></p> <p>The hour (in UTC time) and minute, when the automatic API imports and OSV scanning will run, can be set in the <code>Background tasks</code> section. A restart of the SecObserve instance is required to apply the changes.</p> <p></p>"},{"location":"integrations/osv_scan/#setting-per-product","title":"Setting per product","text":"<p>Only products that have <code>Automatic OSV scanning enabled</code> turned on will be imported automatically.</p> <p></p>"},{"location":"integrations/overview/","title":"Overview","text":"<ul> <li> <p> Authentication</p> <p>All <code>OpenID Connect</code> providers are supported for authentication with an external user directory.</p> <p> OpenID Connect</p> </li> <li> <p> Components</p> <p>Components can be uploaded from CycloneDX and SPDX SBOMs for vulnerability scanning and license management.</p> <p> Upload SBOM</p> </li> <li> <p> Vulnerabilities</p> <p>Vulnerability data can be imported from the results of several vulnerability scanners. Additionally components can be scanned by SecObserve against the OSV database.</p> <p> Supported scanners</p> <p> OSV scan</p> </li> <li> <p> EPSS Scores, Exploits</p> <p>Observations with a CVE Id are enriched with EPSS scores and information about exploits. The necessary data is imported automatically every night.</p> <p> EPSS scores</p> <p> Exploit information</p> </li> <li> <p> Licences, License Groups</p> <p>The list of SPDX licenses is updated nightly. Additionally superusers can manually import license groups generated from the ScanCode LicenseDB.</p> <p> License data</p> </li> <li> <p> VEX Import/Export</p> <p>Vulnerability Exploitability eXchange (VEX) documents can be imported and exported in CSAF, CycloneDX and OpenVEX format.</p> <p> VEX documents</p> </li> <li> <p> Observation and License Export</p> <p>Observations and licenses of a product or product group can be exported to CSV or MS Excel files.</p> <p> Export of observations</p> </li> <li> <p> Issues</p> <p>SecObserve supports automatic creation of issues in GitHub, GitLab and Jira (Cloud).</p> <p> Issue trackers</p> </li> <li> <p> Code links</p> <p>For observations originating from a source file, a link can be generated to view it in the source code repository.</p> <p> Source code repositories</p> </li> <li> <p> Notifications</p> <p>SecObserve can send notifications via email, MS Teams or Slack when a security gate changes or an exception occurs.</p> <p> Notifications</p> </li> <li> <p> Information links</p> <p>Observations and components show links to get further information from external sources.</p> <p> Links to additional information</p> </li> <li> <p> REST API</p> <p>SecObserve is build with an API first approach, every functionality needed to use SecObserve  is covered by the REST API.</p> <p> REST API</p> </li> </ul>"},{"location":"integrations/rest_api/","title":"REST API","text":"<p>SecObserve is build with an API first approach, every functionality needed to use SecObserve is covered by the REST API.</p>"},{"location":"integrations/rest_api/#authentication","title":"Authentication","text":""},{"location":"integrations/rest_api/#jwt","title":"JWT","text":"<p>JWT authentication is used by SecObserve's frontend.</p> Endpoint <code>/api/authentication/authenticate/</code> Validity duration for regular users 7 days / 168 hours  <sup>1)</sup> Validity duration for superusers 1 day / 24 hours  <sup>1)</sup> HTTP header <code>Authorization: JWT</code><code>token</code> <p><sup>1)</sup> Values can be changed by the administrators.</p> <p>A secret is stored in the database that is used to generate the JWT token. The secret can be reset to a new value with a button in the settings:</p> <p></p> <p>After a confirmation dialog, this will invalidate all existing tokens and users have to log in again.</p>"},{"location":"integrations/rest_api/#api-token","title":"API token","text":"<p>API tokens are used for other integration scenarios, e.g. to call the REST API from a CI/CD pipeline to import observations.</p> Validity Until revokation HTTP header <code>Authorization: APIToken</code><code>token</code> <p>API tokens can be created for a product or a user.</p>"},{"location":"integrations/rest_api/#product-api-token","title":"Product API token","text":"<p>A role (see Roles and permissions) must be selected during creation of a product API token, to determine the permissions of the API token for the product.</p> <p></p> <p>The API token can be seen only once after it has been created. It must be copied to ensure that it is not lost.</p> <p></p> <p>Only one API token can be created per product. If it needs to be replaced, it must be revoked first.</p> <p></p>"},{"location":"integrations/rest_api/#user-api-token","title":"User API token","text":"<p>An API token for a user can only be created and revoked with API calls. The token can be seen only once, when it is created. Afterwards there is no way to see that API token again. If it is lost it needs to be revoked and a new one has to be created, as only one API token is allowed per user.</p> <p>The API token has the same permissions for the same products as the user.</p> Endpoint to create API token <code>/api/authentication/create_api_token/</code> Endpoint to revoke API token <code>/api/authentication/revoke_api_token/</code>"},{"location":"integrations/rest_api/#interactive-api-documentation","title":"Interactive API documentation","text":"<p>The full documentation of the REST API is available at <code>&lt;BACKEND_URL&gt;/api/oa3/swagger-ui</code>.</p>"},{"location":"integrations/source_code_repositories/","title":"Source code repositories","text":"<p>Observations can have a source file plus start and end lines as an origin. During the assessment of the observation it is often helpful to view the source code.</p>"},{"location":"integrations/source_code_repositories/#setting-the-repository-in-a-product","title":"Setting the repository in a product","text":"<p>When creating or editing a product, the field <code>Repository prefix</code> can be set. This needs to be the prefix of the URL to show a file in the repository. </p> <ul> <li>If the observations of the product have the branch set, then for GitLab it is something like <code>https://gitlab.maibornwolff.de/secobserve/secobserve/-/blob</code>, for GitHub it looks like <code>https://github.com/MaibornWolff/codecharta/blob</code>. </li> <li>If the observations don't have the branch set, then a branch need to be at the end of the repository prefix, e.g. <code>https://gitlab.maibornwolff.de/secobserve/secobserve/-/blob/dev</code> or <code>https://github.com/MaibornWolff/codecharta/blob/dev</code>.</li> <li>Azure DevOps does not need the branch in the repository prefix, an example is <code>https://dev.azure.com/maibornwolff/SecObserve/_git/SecObserve_Frontend</code>. If the observations have a branch set, then this branch will be used in the URL, otherwise the default branch of the repository will be used.</li> </ul> <p></p>"},{"location":"integrations/source_code_repositories/#showing-the-link","title":"Showing the link","text":"<p>If the <code>Repository prefix</code> is set in the product and the observation has a source file as an origin, then name of the source file will be shown as a link to the source in the repository.</p> <p></p>"},{"location":"integrations/supported_scanners/","title":"Supported scanners","text":""},{"location":"integrations/supported_scanners/#types","title":"Types","text":"<p>There are different types of vulnerability scans:</p> <ul> <li>SCA / Software Composition Analysis: Modern systems are not completely rewritten from scratch, but many basic functions are used as libraries. This applies not only to application code, but in the case of Docker, also to operating system functions and programmes. All these components can have known vulnerabilities that can be exploited by attackers.</li> <li>SAST application / Static Application Security Testing: Many problems can be detected in the code through rule-based searches, e.g. injections or weak encryption. Tools exist for all common programming languages.</li> <li>SAST infrastructure: Also for Infrastructure as Code (Dockerfile, Helm Charts, Terraform, ...) many problems can be found with rule-based searches before applying the code to set up the infrastructure.</li> <li>Secrets: Secrets such as passwords or API keys must not be checked into repositories with the code, and there are tools that search, for example, Git repositories across the entire version history for such secrets.</li> <li>DAST / Dynamic Application Security Testing:: This class is black-box security testing where the tests are performed by attacking an application (typically web applications or APIs) from the outside. The tests can be passive, where only anomalies are looked for, or active attacks on the system. </li> <li>Cloud infrastructure: The running infrastructure, e.g. a Kubernetes cluster, can also be checked for vulnerabilities both with internal views (tests that run inside the infrastructure) and external views (tests from outside via the internet).</li> <li>IAST / Interactive Application Security Testing: IAST works inside an application by instrumenting the code to detect and report issues while the application is running.</li> </ul>"},{"location":"integrations/supported_scanners/#data-formats","title":"Data formats","text":"<p>While every vulnerability scanner writes its own format, there are 2 standardized formats that are implemented by several scanners:</p> <ul> <li> <p>CycloneDX: CycloneDX is a Software Bill of Material (SBOM), that contains information about components of a system and their vulnerabilities. It is typically used by SCA scanners. A CycloneDX file can either be imported to get vulnerabilities (Import vulnerabilities) or to get components with their licenses (Upload SBOMs).</p> </li> <li> <p>SARIF: The Static Analysis Results Interchange Format is an OASIS standard which is implemented by a lot of SAST scanners.</p> </li> </ul> <p>This means the <code>CycloneDX</code> and <code>SARIF</code> parsers can import data from a variety of vulnerability scanners, while other vulnerability scanners need a dedicated parser for their special data format.</p>"},{"location":"integrations/supported_scanners/#scanners","title":"Scanners","text":"<p>These scanners have been tested with SecObserve:</p> Scanner Parser Source SCA Dependency Track Dependency Track API Dependency Check SARIF <sup>1)</sup> File Grype CycloneDX File Trivy CycloneDX File SAST application Bandit SARIF File ESLint SARIF File Find-Sec-Bugs SARIF File Semgrep SARIF File SAST infrastructure Checkov SARIF File KICS SARIF File tfsec SARIF File Trivy SARIF File Secrets Gitleaks SARIF File Trivy SARIF File DAST CryptoLyzer <sup>2)</sup> CryptoLyzer File DrHeader DrHeader File ZAP ZAP File Cloud infrastructure Azure Defender for Cloud <sup>3)</sup> Azure Defender File Prowler 3 Prowler 3 File Prowler 4 OCSF (Open Cybersecurity Schema Framework) File Trivy Operator Prometheus JSON API <p><sup>1)</sup> This is the exception to the rule. Even though SARIF is more suited for static code analysis, it works for Dependency Check.</p> <p><sup>2)</sup> The CryptoLyzer parser checks the results (TLS versions, cipher suites, elliptic curves and signature algorithms) against BSI (Bundesamt f\u00fcr Sicherheit in der Informationssicherheit) recommendations.</p> <p><sup>3)</sup> The results of Azure Defender for Cloud have to be exported manually in CSV format from the Azure Portal.</p> <p>GitHub actions and GitLab CI templates support running vulnerability checks and importing the results into SecObserve via GitHub workflows or GitLab CI pipelines in an efficient way.</p>"},{"location":"integrations/vex/","title":"VEX documents","text":"<p>A VEX (Vulnerability Exploitability eXchange) document is a form of a security advisory that indicates whether a product or products are affected by a known vulnerability or vulnerabilities. SecObserve supports the export and import of VEX documents in three formats:</p> <ul> <li>The Common Security Advisory Framework (CSAF) format is an OASIS standard</li> <li>CycloneDX is an international standard for Bill of Materials (ECMA-424).</li> <li>OpenVEX is a community-driven format, maintained by an OpenSSF special interest group</li> </ul>"},{"location":"integrations/vex/#feature-flag","title":"Feature flag","text":"<p>The VEX feature is is disabled by default because not all installations will use it. It can be enabled by setting the feature flag <code>FEATURE_VEX</code> in the Settings:</p> <p></p> <p>Enabling the feature flag will make the VEX functionality available in the main navigation bar and enable the \"VEX justification\" field in several places.</p>"},{"location":"integrations/vex/#export-a-new-vex-document","title":"Export a new VEX document","text":"<p>To export a VEX document, the user has to define a set of attributes in a dialog. Some of them are different per format,other attributes are common to all formats:</p> Attribute Optionality Description <code>Product</code> optional If a product is selected, only vulnerabilities for that product will be included in the VEX document. <code>Vulnerabilities</code> optional Zero or more names of vulnerabilities to be included in the VEX document, e.g. <code>CVE-2021-44228</code> <code>Branches / Versions</code> optional If a product is selected, the VEX document can be limited to cover only the selected branches / versions of this product. <code>ID prefix</code> mandatory (Only CSAF and OpenVEX) The <code>ID prefix</code> is part of of the unique identifier of the VEX document. The unique id has the format <code>prefix_year_counter</code>, where the counter is increased by 1 for every new document per prefix and year. This unique id will stored in a designated attribute in the VEX document and used for the filename of the VEX document. <p>Either a product or at least one vulnerability has to be selected.</p> <p>After pushing the Create button in the dialog, the VEX document with the version 1 will be created and ready for download. Additionally a database entry with all attributes is created, which can be selected later to export an updated version of the VEX document.</p> <p>Exporting CycloneDX VEX documents</p> <p>CycloneDX VEX documents can only be exported as dedicated BOMs. This means the components with vulnerabilities are not listed in the document but are referenced with BOM-links. Therefore all vulnerabilities to be exported need to have BOM-link, which means either they have been imported via a CycloneDX document or they have been found by the OSV scanner after uploading components with a CycloneDX SBOM (see SBOM workflow).</p> <p>Mapping of justifications</p> <p>Justifications have different enumerations for CSAF/OpenVEX compared to CycloneDX. When entering a justification, e.g. for an observation assessment, a parameter in the settings (see Feature flag), decides if the user gets the CSAF/OpenVEX list or the CycloneDX list of justifications.  E.g. if an organisation decides to publish CycloneDX VEX documents, this attribute should be set to <code>CycloneDX</code>. Then the user can only enter justifications, that can be written directly to the VEX document.</p> <p>If a VEX document is exported in a format that does not correlate with this parameter, the justifications are mapped to corresponding values of the exported format.</p>"},{"location":"integrations/vex/#export-an-updated-vex-document","title":"Export an updated VEX document","text":"<p>After selecting the entry of either a CSAF or OpenVEX document from the respective list, a form shows the details of its attributes and a button to update a document. Some of the attributes can be changed for a new version of the document. If there have been no changes to the included vulnerabilities, no new document will be created. Otherwise a new version of the document will be created and ready for download.</p>"},{"location":"integrations/vex/#import-vex-documents","title":"Import VEX documents","text":"<p>VEX documents can be imported in CSAF, OpenVEX and CycloneDX (integrated and dedicated) format. After importing the file, the document will be parsed and the VEX statements will be applied to the referenced observations. A user needs to be <code>superuser</code> to import VEX documents. </p> <p>When observations are imported, the VEX statements will be applied to the referenced observations as well.</p> <p>How are the referenced observations determined?</p> <p>For CSAF, OpenVEX and CycloneDX integrated VEX documents, the reference is determined by PURLs. </p> <ul> <li>First, the relevant products are determined by the product PURL. The PURL of the product or the PURL of a branch must match the product PURL in the VEX statements.  </li> <li>Second, the relevant observations are determined by their Vulnerability ID and optionally the component PURL. The Vulnerability ID of the observation must be the same as the Vulnerability ID of the VEX statements. If the VEX statement contains a component PURL, this must match the vulnerability PURL in the component PURL of the observation.</li> </ul> <p>For CycloneDX dedicated VEX documents, the reference is determined by a BOM-link. These documents contain only vulnerability and VEX information with references to components in a separate SBOM. Therefore it is a precondition to have imported the corresponding SBOM before. Then the observations are determined by the Vulnerability ID and the BOM-link.</p> <p>When do PURLs match?</p> <p>Two PURLs match if:</p> <ul> <li>The <code>type</code> is the same in both PURLs.</li> <li>Both PURLs have the same <code>namespace</code> or there is no <code>namespace</code> in both PURLs.</li> <li>The <code>name</code> is the same in both PURLs.</li> <li>Either both PURLs have the same <code>version</code>, there is no <code>version</code> in both PURLs or there is a <code>version</code> in one PURL but not in the other one.</li> <li>When both PURLs have <code>qualifiers</code>, then all key/value pairs that exist in both PURLs must be the same. When both PURLs do not have <code>qualifiers</code> or some of the keys in one set of <code>qualifiers</code> do not exist in the other PURL, then the PURLs match as well.</li> <li>Either both PURLs have the same <code>subpath</code>, there is no <code>subpath</code> in both PURLs or there is a <code>subpath</code> in one PURL but not in the other one.</li> </ul>"},{"location":"usage/assess_observations/","title":"Assessments, approvals and reviews","text":""},{"location":"usage/assess_observations/#assessment","title":"Assessment","text":"<p>With an assessment of an observation the user can change two attributes of an observation:</p> <ul> <li>The severity given by the parser must not necessarily match the severity of the observation for the current product.</li> <li>All observations have initially the status <code>Open</code>. The result of an investigation how to deal with the observation might say, the observation must not be fixed because it is ...<ul> <li>... <code>In review</code> and needs further investigation.</li> <li>... already <code>Resolved</code>. You have to be aware that the observation will be set back to <code>Open</code> if it will be found in a subsequent import.</li> <li>... a <code>Duplicate</code> of another observation.</li> <li>... a <code>False positive</code> that has been detected by the scanner wrongly.</li> <li>... <code>Risk accepted</code>, a decision that a breach because of that observation can be managed.</li> <li>The system is <code>Not affected</code> because the observation has been mitigated by a measure.</li> </ul> </li> </ul> <p>The dialog to enter the assessment can be opened when showing the observation: </p> <p></p> <p>In the assessment dialog the user can change either the severity and/or the status and has to enter a mandatory comment to explain the change:</p> <p></p> <p>A new entry with the changed values is stored in the <code>Observation Log</code> after the assessment has been saved.</p>"},{"location":"usage/assess_observations/#approvals","title":"Approvals","text":"<p>With the default settings of the product, the assessment is activated right away. If more control is needed, the product can be configured to require an approval before the assessment is activated. This can be done while creating or editing a product:</p> <p></p> <p>The setting is also available for product groups. If it is set for a product group, it will be inherited by all products in that group.</p> <p>If the approval is required, the dialog showing the observation or and the dialog showing the observation log (after clicking on an entry in the list of observation logs) will show a button to either approve or reject the assessment:</p> <p></p> <p>Be aware, that the user who has created the assessment is not allowed to approve or reject it. The approval must be done by another user.</p> <p></p>"},{"location":"usage/assess_observations/#reviews","title":"Reviews","text":"<p>To make it easier to find observations with the status <code>In Review</code> or assessements needing an approval, a tab is shown for the product, if reviews or approvals are pending:</p> <p></p>"},{"location":"usage/branches/","title":"Branches and Versions","text":"<p>Branches and versions are an optional feature of the product. They can be used to separate the observations for different branches or versions of a product. If only one branch or version is used to develop a product, this feature can be ignored.</p>"},{"location":"usage/branches/#list-of-branches-and-versions","title":"List of branches and versions","text":"<p>A product has a list of branches / versions. They can either be created manually from the Branches / Versions tab of the product or will be created automatically, when observations are imported using a branch / version name that didn't exist before for that product.</p> <p></p> <p>The list of branches / versions shows the severities of open observations for each branch / version.</p> <p>Clicking on the name of a branch / version brings up the list of open observations for that branch / version.</p> <p>Warning</p> <p>When a branch / version is deleted, all observations for that branch will be deleted as well.</p>"},{"location":"usage/branches/#default-branch-version","title":"Default branch / version","text":"<p>The Default branch / version should always be set, when branches / versions are used for the observations.</p> <ul> <li>The metrics on the dashboard and on the Metrics tab are calculated using the observations where the default branch / version is set.</li> <li>The number of severities in the header when showing a product are for the observations where the default branch / version is set as well.</li> <li>Issues in GitHub, GitLab or Jira are created only for the default branch / version.</li> <li>The default branch / version cannot be deleted and is exempt from the housekeeping.</li> </ul> <p>The default branch / version can be set manually while editing a product. If it is not set manually, it will be set automatically with the first branch / version that is created, either after importing observations with a branch / version name or by manually creating a branch / version.</p> <p></p> <p>The Observations tab shows a button to show all open observations for the default branch / version.</p> <p></p>"},{"location":"usage/branches/#housekeeping","title":"Housekeeping","text":"<p>Inactive branches / versions will be deleted automatically after a certain time. Inactivity is defined as the number of days since the last import of observations for a branch / version.</p>"},{"location":"usage/branches/#parameters","title":"Parameters","text":"<p>The parameters are set globally in the Settings and can be partially overridden per product.</p> Parameter global Description BRANCH_HOUSEKEEPING_CRONTAB_MINUTE Minutes crontab expression for branch / version housekeeping BRANCH_HOUSEKEEPING_CRONTAB_HOUR Hours crontab expression for branch / version housekeeping (UTC) BRANCH_HOUSEKEEPING_ACTIVE If this parameter is set, inactive branches / versions will be deleted automatically. BRANCH_HOUSEKEEPING_KEEP_INACTIVE_DAYS Days before incative branches / versions and their observations are deleted BRANCH_HOUSEKEEPING_EXEMPT_BRANCHES Regular expression which branches / versions to exempt from deletion <p>Per default the task to delete inactive branches / version including their observations is scheduled to run every night at 02:00 UTC time. This default can be changed by administrators via the Background tasks section in the Settings. Hours are always in UTC time.</p> <p></p>"},{"location":"usage/branches/#product-specific-settings","title":"Product specific settings","text":"<p>A product can override the housekeeping behaviour by setting the <code>Housekeeping</code> attribute:</p> <ul> <li>Standard: Use the instance-wide definition, this is the default.</li> <li>Disabled: Do not delete inactive branches for that product.</li> <li>Product specific: Use product specific settings for deletion of inactive branches.</li> </ul> <p></p>"},{"location":"usage/branches/#protect-branches","title":"Protect branches","text":"<p>A branch can be proceted to prevent it from being deleted by the housekeeping task. This can be done by setting the <code>Protect from housekeeping</code> attribute of a branch.</p>"},{"location":"usage/duplicates/","title":"Duplicates","text":"<p>Duplicate observations can appear when different scanners are used to scan the same code or dependencies. Sometimes one scanner might produce potential duplicates as well, e.g. reporting the same vulnerability for multiple dependencies.</p>"},{"location":"usage/duplicates/#identify-potential-duplicates","title":"Identify potential duplicates","text":"<p>An asynchronous process to detect potential duplicates is executed when importing scan results. It checks all existing open observations for the same project, the same branch / version and the same service plus one of two conditions:</p> <ul> <li>The same title for different components</li> <li>Observations with the same source file name and start line number from different scanners</li> </ul> <p>The list of observations in the user interface shows if an observation has potential duplicates.</p>"},{"location":"usage/duplicates/#mark-duplicates","title":"Mark duplicates","text":"<p>If an observation has potential duplicates, they are shown in a list when showing the observation details. The user can tick one or more of them and mark them as duplicates. This will add an assessment to the observation with the status <code>DUPLICATE</code> and a comment.</p>"},{"location":"usage/import_observations/","title":"Import observations","text":""},{"location":"usage/import_observations/#import-from-ci-pipelines-via-the-api","title":"Import from CI pipelines via the API","text":"<p>GitHub actions and GitLab CI templates support running vulnerability checks and importing the results into SecObserve via GitHub workflows or GitLab CI pipelines in an efficient way.</p>"},{"location":"usage/import_observations/#import-from-the-frontend","title":"Import from the frontend","text":"<p>Alternatively observations can be imported via the user interface. When showing a product, there are buttons to either upload a file or to import from an API:</p> <p></p>"},{"location":"usage/import_observations/#upload-of-files","title":"Upload of files","text":"<p>A file needs to be selected. The parser to interpret the content of the file will be detected automatically. Optional are attributes for the branch / version, service, docker image, endpoint URL and Kubernetes cluster.</p> <p>When uploading a CycloneDX file here, only the vulnerabilities will be imported. To import all components with their licenses, Upload SBOM has to be used.</p>"},{"location":"usage/import_observations/#api-import","title":"API import","text":"<p>Before importing observations from an API, an API configuration needs to be created for the product. This API configuration specifies how to access the API (URL, API key,Query, Basic Authentication, SSL Verify, ...). Optional for the import are attributes for the branch / version, service, docker image, endpoint URL and Kubernetes cluster.</p>"},{"location":"usage/import_observations/#import-algorithm","title":"Import algorithm","text":"<p>The import algorithm has to decide, if an observation already exists and needs to be updated or it is new and needs to be created. But how does the import algorithm identifies an observation to make this decision? Two terms help to understand how that works:</p> <ul> <li>Identity hash: The <code>identity hash</code> is a SHA256 hash code of the concatenation of the observation's title and all its origins <sup>[1]</sup>. Two observations with the same <code>identity hash</code> are defined as identical.</li> <li>Vulnerability check: An import for one product, one branch / version, one service and one file name resp. one API configuration is a so-called vulnerability check.</li> </ul> <p>A flowchart visualizes the import algorithm:</p> <pre><code>    flowchart TD\n    read(Read observations for a vulnerability check) --&gt; all_observations{For all observations of this vulnerability check}\n    all_observations -- with observation --&gt; identity{Identical observation in the same vulnerability check?}\n    subgraph Create or update\n    identity -- yes --&gt; update(Update existing observation)\n    identity -- no --&gt; create(Create new observation)\n    end\n    all_observations -- finished ----&gt; resolved(Set status to **Resolved** for all untouched observations of this vulnerability check)\n</code></pre> <p>[1]: The tag of the docker image is not part of the <code>identity hash</code> to allow updates of the docker image without creating a new observation.</p>"},{"location":"usage/import_observations/#initial-status-of-observations","title":"Initial status of observations","text":"<p>The initial status of imported observations is <code>Open</code> by default. If the attribute Status \"In review\" for new observations in a Product or a Product Group is set to <code>true</code>, the initial status of new observations will be <code>In review</code> for the respective Product or all Products of the Product Group.</p>"},{"location":"usage/license_management/","title":"License management","text":""},{"location":"usage/license_management/#activating-deactivating-license-management","title":"Activating / deactivating license management","text":"<p>License management is activated by default. If it is not used in an organization it can be be deactivted via a feature flag in the Settings.</p> <p></p> <p>If license management is deactivated:</p> <ul> <li>The <code>Licenses</code> menu is not visible in the navigation.</li> <li>The automatic import of SPDX licenses is deactivated.</li> <li>Licenses for components are not imported from CycloneDX or SPDX files and the <code>License</code> tab is not visible in the Product view.</li> </ul>"},{"location":"usage/license_management/#managing-licenses-in-products","title":"Managing licenses in products","text":""},{"location":"usage/license_management/#importing-components-with-licenses","title":"Importing components with licenses","text":"<p>When uploading data from CycloneDX or SPDX SBOMs, the licenses of the components are imported as well, if they are available in the SBOM. The components and licenses are shown in the <code>License</code> tab of the Product view.</p> <p></p> <p>After clicking on an entry, the details of the component and its license are shown.</p> <p></p>"},{"location":"usage/license_management/#declared-and-concluded-licenses","title":"Declared and concluded licenses","text":"<p>Declared licenses and concluded licenses represent two different stages in the licensing process within software development.</p> <ul> <li>Declared licenses refer to the initial intention of the software authors regarding the licensing terms under which their code is released. </li> <li>Concluded licenses on the other hand, are the result of a comprehensive analysis of the project's codebase to identify and confirm the actual licenses of the components used, which may differ from the initially declared licenses. </li> </ul> <p>While declared licenses provide an upfront indication of the licensing intentions, concluded licenses offer a more thorough understanding of the actual licensing within a project, facilitating proper compliance and risk management.</p> <p>(Copied from the CycloneDX specification)</p> <p>Both types of acknowledgement are imported from an SBOM and shown in the UI. If a CycloneDX SBOM doesn't include a license acknowledgement, it will be treated as <code>Declared</code>.</p> <p>Additionally, a user can manually add a concluded license, with the button <code>ADD / EDIT CONCLUDED LICENSE</code>, for example when there was no license or there was a wrong license in the SBOM.</p> <p>There is a priority for the license acknowledgement. </p> <ol> <li>If a manual concluded license has been set, this is used for the evaluation of the license and is shown in lists.</li> <li>The imported concluded license is used, if it is available and no manual concluded license is set.</li> <li>Otherwise the imported declared license is used for evaluation and lists if it was in the SBOM.</li> </ol> <p>Manually set concluded licenses are stored in a cache and will be set again for a component, when a new SBOM is imported for the same product or another product in the same product group.</p>"},{"location":"usage/license_management/#evaluation-of-licenses","title":"Evaluation of licenses","text":"<p>A License Policy for the Product can be set, when editing the product settings.</p> <p></p> <p>If no License Policy is set, all licenses are evaluated as <code>Unknown</code>. If a License Policy is set, the licenses are evaluated according to the policy:</p> <ul> <li>Allowed: There is no problem using the component with that license.</li> <li>Forbidden: Using the component with that license might lead to legal problems and the component cannot be used for the Product.</li> <li>Review: The license shall be reviewed and the License Policy shall be updated after the review.</li> <li>Unknown: The license is not included in the License Policy.</li> <li>Ignored: The component is not relevant for the license management.</li> </ul> <p>License expressions are evaluated by their included licenses, if the operators are all either <code>AND</code> or <code>OR</code>. If other operators are used, e.g. <code>WITH</code>, the expression is evaluated as <code>Unknown</code>, if there is no explicit rule for this license expression.</p> <p>If multiple licenses have been found for a component, they are evaluated like an <code>AND</code> expression. If for example one license is <code>Allowed</code> and the other one is <code>Forbidden</code>, the component is evaluated as <code>Forbidden</code>. </p> <p>A good strategy is to start with an existing License Policy and when needed make a copy of it and adjust the rules to the needs of the Product.</p>"},{"location":"usage/license_management/#managing-license-policies","title":"Managing License Policies","text":"<p>A <code>License Policy</code> defines the rules for the usage of licenses in a Product. </p> <p>The list of <code>License Policies</code> can be found in the <code>Licenses</code> sub-menu under <code>Administration</code>.</p> <p>A <code>License Policy</code> can have another license policy as a <code>Parent</code>. If a license policy has a parent, the rules of the parent are also valid for the child policy, but existing rules of the parent can be overriden and new rules can be added. A license policy which is a parent cannot have a parent itself.</p> <p>Within the <code>License Policy</code> itself a comma-separated list of component types (e.g. <code>apk</code> or <code>deb</code>) can be defined, which shall be ignored in the license evaluation. This can be useful for operating system packages in a Docker container, which are not relevant for the license management.</p> <p>The attribute <code>Public</code> defines, if the License Policy is visible for all users or only for the members of the policy.</p> <p>Actions</p> <ul> <li>The <code>Export</code> button opens a sub-menu to exports the License Policy either as a SecObserve specific JSON or YAML file or in a format that can be used with sbom-utility, using the <code>--config-license</code> parameter. The SecObserve specific output format is specified as a JSON schema.</li> <li>With the <code>Apply</code> button the rules of the License Policy are applied to all products, that have this License Policy set.</li> <li>The <code>Copy</code> button creates a new License Policy with the same rules, which can be adjusted for a specific Product.</li> </ul> <p></p> <p>A <code>License Policy</code> has a list of items, which are the rules of the policy. It can be </p> <ul> <li>a rule for a License Group or</li> <li>a rule for a specific SPDX license, </li> <li>a rule for a license expression or</li> <li>a rule for an non-spdx license string, e.g. a license that is not in the SPDX list or a license expression.</li> </ul> <p></p> <p>Additionally a <code>License Policy</code> has a list of user members and a list of authorization group members, which define who has access to a license policy, either read-only or as a manager. To define read-only members is not necessary, if the policy is defined as <code>Public</code>. Additionally, users can view all license policies that are assigned to a product, if they have access to the product.</p> <p></p>"},{"location":"usage/license_management/#managing-license-groups","title":"Managing License Groups","text":"<p>A <code>License Group</code> is a collection of licenses with similar license conditions. There is a predefined list of license groups, taken from the classification of the Blue Oak Council. Administrators can import license group from the ScanCode LicenseDB, see License data import.</p> <p>As with <code>License Policies</code>, a <code>License Group</code> </p> <ul> <li>can be found in the <code>Licenses</code> sub-menu under <code>Administration</code>,</li> <li>can be copied if adjustments are needed for a specific Product,</li> <li>can be public, so that all users can see the group and its licenses,</li> <li>has a list of user members and a list of authorization group members, which define who has access to a license group, either read-only or as a manager.</li> <li>users can view all license groups that are assigned to a license policy which is assigned to a product, if they have access to the product.</li> </ul>"},{"location":"usage/metrics/","title":"Metrics","text":"<p>Metrics about observations are shown in 2 places:</p> <ul> <li>On the dashboard a user can see the aggregated metrics of the observations for all products they have access to.</li> <li>On the Metrics tab of a product a user can see the metrics for the observations of that product.</li> </ul> <p></p> <p>Metrics are calculated in an asychronous task. Per default the task is scheduled to run every 5 minutes. This default can be changed by administrators via the Settings.</p>"},{"location":"usage/product_groups/","title":"Product groups","text":"<p>A product group is a collection of products. It is used to group products that belong together, e.g. because they are part of the same project.</p> <p>Users cannot see only the list of the associated products and their combined metrics, but there are some settings that are shared between all products of the group:</p> <ul> <li>Rules defined for a product group are applied to all products in the group in addition to the rules defined for the product.</li> <li>Members defined for a product group have access to the products in the group in addition to the members defined for the product.</li> <li>The API token of a product group can be used to access the API for all products in the group.</li> <li>Housekeeping for branches / versions:<ul> <li>Standard: The branches / versions of all products in the group are deleted according to the settings of the product.</li> <li>Disabled: Housekeeping for branches / versions is disabled for all products in the group.</li> <li>Product group specific: The branches / versions of all products in the group are deleted according to the settings of the product group.</li> </ul> </li> <li>The settings for Notifications are used, if no notification settings are defined for the product. If there are notification settings defined for the product, they override the settings of the product group.</li> <li>Security gates:<ul> <li>Standard: The security gates of all products in the group are calculated according to the settings of the product.</li> <li>Disabled: Security gates are disabled for all products in the group.</li> <li>Product group specific: Security gates of all products in the group are calculated according to the settings of the product group.</li> </ul> </li> </ul>"},{"location":"usage/risk_acceptance_expiry/","title":"Risk acceptance expiry","text":"<p>When an observation gets the status <code>Risk accepted</code>, it is possible to set an expiry date for the acceptance. After the expiry date, the assessment with the risk acceptance will automatically be removed.</p> <p>The number of days until the expiry date can be configured on 3 different levels:</p> <ol> <li>In the Settings is the parameter <code>Risk acceptance expiry (days)</code>, which is used if there are no specific settings for Products or Product Groups.</li> <li>Product Groups have the parameter <code>Risk acceptance expiry</code>, which can have 3 values:<ul> <li>Standard: The standard settings are used for all Products, if nothing specific is configured for a Product.</li> <li>Disabled: No expiry date is set for the risk acceptance, if nothing specific is configured for a Product.</li> <li>Product group specific: Use a Product Group specific value for <code>Risk acceptance expiry (days)</code>, if nothing specific is configured for a Product.</li> </ul> </li> <li>Products have the parameter <code>Risk acceptance expiry</code> as well, which can have 3 values:<ul> <li>Standard: The standard settings or Product Group specific settings are used for the Product.</li> <li>Disabled: No expiry date is set for the risk acceptance.</li> <li>Product specific: Use a Product specific value for <code>Risk acceptance expiry (days)</code>.</li> </ul> </li> </ol> <p>A value of <code>0</code> for the <code>Risk acceptance expiry (days)</code> means that no expiry date is set for the risk acceptance.</p> <p>The number of <code>Risk acceptance expiry (days)</code> will be used to set a default for the expiry date when the status <code>Risk accepted</code> is set in an assessment or when manually editing an Observation. This default can be changed or set to empty in the respective dialogs. If a Product Rule or General Rule sets the status of an Observation to <code>Risk accepted</code>, the default expiry date will be set as well.</p> <p>Per default the task to check the risk acceptance expiry is scheduled to run every night at 01:00 UTC time. This default can be changed by administrators via the Background tasks section in the Settings.  Hours are always in UTC time.</p>"},{"location":"usage/rule_engine/","title":"Rule engine","text":"<p>Sometimes the result of a scanner doesn't fit to the product's needs. Either the severity or the status need to be adjusted. To avoid having to do many manual assessments regularly, a built-in rule engine can adjust severity and/or status directly with the import of observations.</p> <p>This can remove a lot of noise, for example by setting observations to <code>False positive</code>, in case the ruleset of the scanner can not be adjusted appropriately. </p>"},{"location":"usage/rule_engine/#rules","title":"Rules","text":"<p>Rules can be managed in two ways:</p> <ul> <li>General rules will be applied for all products. A product can be excluded from general rules in its settings.</li> <li>Product rules are only valid for one product or for all products of a product group.</li> </ul> <p>These fields are used to decide if a rule shall be applied for an observation:</p> <ul> <li>Parser (mandatory): The observation has been imported with this parser.</li> <li>Scanner prefix (optional): The observation has been generated by a scanner which name starts with this prefix. A prefix is used here because the scanner field in the observation often contains the version of the scanner as well, which is typically irrelevant for the rule.</li> <li>Observation title (optional): Regular expression to match the observation's title</li> <li>Origin component name:version (optional): Regular expression to match the component name:version</li> <li>Origin docker image name:tag (optional): Regular expression to match the docker image name:tag</li> <li>Origin endpoint URL (optional): Regular expression to match the endpoint URL</li> <li>Origin service name (optional): Regular expression to match the service name</li> <li>Origin source file (optional): Regular expression to match the source file</li> <li>Origin cloud qualified resource (optional): Regular expression to match the cloud qualified resource, which is the concatenation of account (AWS) or subscription (Azure) or project (GCP) with the resource</li> </ul> <p>If an observation matches all fields containing a value, than the new severity and/or new status is set in the observation and the rule's description is stored as a comment in the <code>Observation Log</code>.</p>"},{"location":"usage/rule_engine/#approvals","title":"Approvals","text":"<p>With the default settings of the product, the rule will be activated right away if enabled. If more control is needed, an approval can be configured:</p> <ul> <li>For General rules the feature <code>General rules need approval</code> can be set in the Settings.</li> <li>For Product rules the setting <code>Rules need approval</code> can be set while creating or editing a product. The setting is also available for product groups. If it is set for a product group, it will be inherited by all products in that group.</li> </ul> <p>If the approval is required, the dialog showing the rule  will show a button to either approve or reject the assessment:</p> <p></p> <p>Be aware, that the user who has created or edited the rule is not allowed to approve or reject it. The approval must be done by another user.</p> <p></p>"},{"location":"usage/security_gates/","title":"Security gates","text":"<p>A security gate shows, that a product does not exceed a defined amount of vulnerabilities per severity. It can be <code>Passed</code> if the product is under or at the defined thresholds or <code>Failed</code> if the product has more observations for at least one severity.</p> <p></p> <p>There is an instance-wide definition of the thresholds, that can be changed by an administrator. The default is:</p> Severity Threshold Critical 0 High 0 Medium 99999 Low 99999 None 99999 Unknown 99999 <p>A product can decide how to deal with security gates by setting the <code>Security gate</code> attribute:</p> <ul> <li>Standard: Use the instance-wide definition, this is the default.</li> <li>Disabled: Do not calculate and show a security gate.</li> <li>Product specific: Use product specific thresholds to calculate the security gate.</li> </ul> <p></p>"},{"location":"usage/upload_sbom/","title":"Upload SBOMs","text":"<p>Uploading an SBOM for a Product or Branch / Version imports all components with their licenses and dependencies. This is a precondition to use License management or OSV scanning. When uploading a CycloneDX file, no vulnerabilities will be imported, just components with their licenses. To import vulnerabilities from a CycloneDX file, Import observations has to be used.</p> <p>Currently CycloneDX and SPDX files are supported, both in JSON format.</p>"},{"location":"usage/upload_sbom/#import-from-ci-pipelines-via-the-api","title":"Import from CI pipelines via the API","text":"<p>There is a GitHub action and GitLab CI template available to import an SBOM shortly.</p> <p>Alternatively, the REST API can be used to import an SBOM, with the endpoints <code>/api/import/file_upload_sbom_by_id/</code> and <code>/api/import/file_upload_sbom_by_name/</code>.</p>"},{"location":"usage/upload_sbom/#import-from-the-frontend","title":"Import from the frontend","text":"<p>Additionally observations can be imported via the user interface. When showing a product, there is a respective button in the Import menu:</p> <p></p> <p></p> <p>A file needs to be selected. The parser to interpret the content of the file will be detected automatically. The Branch / Version is optional.</p> <p>When uploading a CycloneDX file here, no vulnerabilities will be imported, just components with their licenses. To import vulnerabilities from a CycloneDX file, Import observations has to be used.</p>"},{"location":"usage/upload_sbom/#import-algorithm","title":"Import algorithm","text":"<p>The import algorithm to decide, if a component with its license already exists it is new is similar as for importing observations.</p>"},{"location":"usage/users_permissions/","title":"Users, groups and permissions","text":""},{"location":"usage/users_permissions/#users","title":"Users","text":"<p>SecObserve supports two types of users:</p> <ul> <li>Internally managed users: You need a username and password given by a SecObserve administrator and use the <code>SIGN IN WITH USER</code> button.</li> <li>Users managed in a directory: The button <code>ENTERPRISE SIGN</code> will redirect you to login page of your users directory, if OpenID Connect is configured.</li> </ul> <p>Users have specified permissions depending on their type and role in a product.</p>"},{"location":"usage/users_permissions/#user-types","title":"User types","text":"<p>The user type can be set by flags in the user administration:</p> <ul> <li>Superusers are the administrators of the system.</li> <li>External users do not belong to the organization, e.g. customers or partners.</li> <li>Internal users are all users who are not superusers or external users.</li> </ul> <p>When a user logs in with an OIDC account for the first time, it can be automatically determined whether it is an internal or external user by their email address. The parameter <code>Internal users</code> in the Settings takes a comma-separated list of email of regular expression. If one regular expression matches the email address of the user, the user is considered an internal user. If no regular expression matches, the user is considered an external user.</p> <p>There are some general permissions based on the user's type:</p> Internal External Superuser Create Product Groups X - X Create Product X - X View General Rules X X X Create General Rules - - X Edit General Rules - - X Delete General Rules - - X Change own password <sup>1)</sup> X X X Change all passwords  <sup>1)</sup> - - X Manage users - - X Create authorization groups X - X Manage authorization groups X <sup>2)</sup> X <sup>2)</sup> X Import VEX documents - - X Manage VEX counters - - X Manage Settings - - X Reset JWT secret - - X <p><sup>1)</sup> Not for OIDC users</p> <p><sup>2)</sup> Only if the user is a manager of the authorization group</p>"},{"location":"usage/users_permissions/#authorization-groups","title":"Authorization groups","text":"<p>Authorization groups are used to manage permissions for multiple users at once. Users can be added to authorization groups and the groups can be assigned to products. This way, permissions can be managed centrally for multiple users.</p> <p>Authorization groups can be mapped to OIDC group claims. If the OICD token includes a group claim (see OpenID Connect authentication), the user will automatically be added to the authorization group, where an entry of the group claim matches the attribute <code>oidc_group</code>. If the users will be removed from the group in the user directory, the user will be removed from the authorization group automatically as well.</p>"},{"location":"usage/users_permissions/#roles-and-permissions","title":"Roles and permissions","text":"<p>While superusers have permission to view and edit all data, internal and external users must be either a user member or member of an authorization group with a specific role to access the product and its data. User members and authorization group members of a product group have access to all products of that group with their respective role.</p> Reader Writer Maintainer Owner Upload View Product Group X X X X - Edit Product Group - - X X - Delete Product Group - - - X - View Product X X X X - Import Observations - X X X X Edit Product - - X X - Delete Product - - - X - View Observation X X X X - Create Observation - X X X - Edit Observation <sup>1)</sup> - X X X - Assess Observation - X X X - Delete Observation - - - X - View Product Rules X X X X - Create Product Rules - - X X - Edit Product Rules - - X X - Apply Rules to Product - - X X - Delete Product Rules - - X X - View API Configuration X X X X - Create API Configuration - - X X - Edit API Configuration - - X X - Delete API Configuration - - X X - View User Member X X X X - Create User Member - - X <sup>2)</sup> X - Edit User Member - - X <sup>2)</sup> X - Delete User Member - - X <sup>2)</sup> X - View Authorization Group Member X X X X - Create Authorization Group Member - - X <sup>2)</sup> X - Edit Authorization Group Member - - X <sup>2)</sup> X - Delete Authorization Group Member - - X <sup>2)</sup> X - View VEX X X X X - Create VEX <sup>3)</sup> - - X X - Edit VEX <sup>3)</sup> - - X X - Delete VEX<sup>3)</sup> - - X X - <p><sup>1)</sup> Only manually created observations can be edited</p> <p><sup>2)</sup> Maintainers are not allowed to manipulate Owners of that product</p> <p><sup>3)</sup> Only for VEX documents (CSAF or OpenVEX) linked to a product. For VEX documents without a product, the user who created the VEX document is the owner of the document and can perform all actions on it.</p>"},{"location":"usage/users_permissions/#management-of-users-and-authorization-groups","title":"Management of users and authorization groups","text":"<p>Users and authorization groups can be managed by superusers in the Access Control administration:</p> <p></p>"}]}